{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STXGB Model\n",
    "\n",
    "This notebook contains the code for developing STXGB models. STXGB has three variants (STXGB-FB, STXGB-SG, and STXGB-SGR) and first we define the features that we have used in each variant. Then for each prediction horizon, we train a separate model using XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO4dO2DGcoM8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn\n",
    "import geopandas as gpd\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output directory\n",
    "# You can change it if you want to\n",
    "output_dir = './output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIirtJ6Ojnrk"
   },
   "source": [
    "## 1- Loading Data\n",
    "The `all_features_v1.csv` contains all of the features and target variables that we have used in different variants of STXGB model and for 1- to 4-week prediction horizons. \n",
    "\n",
    "\n",
    "This file is published publicly alongside the code, so you can download the csv file and run STXGB models for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load data from Zenodo URL\n",
    "\n",
    "df_url = 'https://zenodo.org/record/5533027/files/all_features_v1_0.csv?download=1'\n",
    "covid_df = pd.read_csv(df_url,index_col=0, dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of weeks for which we have data\n",
    "covid_df.shape[0]/3103 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQ22baxHhWFJ",
    "outputId": "be3149c5-a9b0-46d8-fea1-3c3a1cd56f3d"
   },
   "outputs": [],
   "source": [
    "covid_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a base geojson file \n",
    "\n",
    "This file contains county FIPS and is used to store model outputs is a georeferenced format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://drive.google.com/file/d/1MVyLzzHl3hzno4o1rLZtI0peqIi23zsr/view?usp=sharing'\n",
    "url_counties='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "counties_shp = gpd.read_file(url_counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now, we have to define feature names for each of the models. Please refer to the methods section of the article to read more about these features.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The list of features in the base model (base features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3REEMacIUqX",
    "outputId": "79faf58e-1a02-433d-c813-db07799203cf"
   },
   "outputs": [],
   "source": [
    "temp_cols = [col for col in covid_df.columns if 'TEMP' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_cols = ['POP_DENSITY',\n",
    "'PCT_MALE',\n",
    "'PCT_65_OVE',\n",
    "'PCT_BLACK',\n",
    "'PCT_HISPAN', \n",
    "'PCT_AMIND',\n",
    "'PCT_RURAL',\n",
    "'PCT_COL_DE' ,\n",
    "'PCT_TRUMP_',\n",
    "'MED_HOS_IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_cols = [col for col in covid_df.columns if 'DELTA_INC' in col]\n",
    "inc_cols.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = socio_cols + temp_cols + inc_cols + ['LOG_MEAN_INC_RATE_T_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The list of features in the STXGB-FB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_cols = [col for col in covid_df.columns if 'DELTA_SPC_T' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_cols = [col for col in covid_df.columns if 'REL_' in col]\n",
    "rel_cols_non_delta = [col for col in rel_cols if 'DELTA' in col]\n",
    "rel_cols = list(set(rel_cols)^set(rel_cols_non_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_cols = [col for col in covid_df.columns if 'RATIO_' in col]\n",
    "ratio_cols_non_delta = [col for col in ratio_cols if 'DELTA' in col]\n",
    "ratio_cols = list(set(ratio_cols)^set(ratio_cols_non_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_features = socio_cols + temp_cols + rel_cols + ratio_cols + spc_cols  + inc_cols  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_features.extend(('LOG_MEAN_INC_RATE_T_4', 'MEAN_SPC_T_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. The list of features in the STXGB-SG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpc_cols = [col for col in covid_df.columns if 'DELTA_FPC_T' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grfPd6apJkha",
    "outputId": "c3293cdf-2ed9-4855-a512-62bcbf5e7029"
   },
   "outputs": [],
   "source": [
    "pct_home_cols = [col for col in covid_df.columns if 'completely_home_' in col]\n",
    "pct_home_cols_non_base = [col for col in pct_home_cols if 'baselined' in col]\n",
    "pct_home_cols = list(set(pct_home_cols)^set(pct_home_cols_non_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DESQg6ZWJ35_",
    "outputId": "720a95b0-9971-49c4-e734-6e74fc14846a"
   },
   "outputs": [],
   "source": [
    "dist_traveled_cols = [col for col in covid_df.columns if 'distance_traveled_' in col]\n",
    "dist_traveled_cols_non_current = [col for col in dist_traveled_cols if 'current' in col]\n",
    "dist_traveled_cols = list(set(dist_traveled_cols)^set(dist_traveled_cols_non_current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_kXAjn7Hj68"
   },
   "outputs": [],
   "source": [
    "safegraph_features = socio_cols + temp_cols + pct_home_cols + dist_traveled_cols + \\\n",
    "                    fpc_cols + inc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_features.extend(('LOG_MEAN_INC_RATE_T_4','MEAN_FPC_T_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. The list of features in the STXGB-SGR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xU-x5DZH4w9",
    "outputId": "315f2cbf-3c01-401a-dfc7-d24c25c6abb9"
   },
   "outputs": [],
   "source": [
    "baselined_cols = [col for col in covid_df.columns if 'baselined_' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpIJZ-agIEeK",
    "outputId": "e63e2864-d36c-4c05-f39e-f3ee1a68ca9f"
   },
   "outputs": [],
   "source": [
    "slope_cols = [col for col in covid_df.columns if 'slope_' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_full_features = safegraph_features + baselined_cols + slope_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_full_features = set(safegraph_full_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " features_to_remove=['pct_completely_home_device_count_current_T_1',\n",
    " 'pct_completely_home_device_count_current_T_2',\n",
    " 'pct_completely_home_device_count_current_T_3',\n",
    " 'pct_completely_home_device_count_current_T_4',\n",
    " 'pct_delivery_behavior_devices_baselined_T_1',\n",
    " 'pct_delivery_behavior_devices_baselined_T_2',\n",
    " 'pct_delivery_behavior_devices_baselined_T_3',\n",
    " 'pct_delivery_behavior_devices_baselined_T_4',\n",
    " 'pct_delivery_behavior_devices_slope_T_1',\n",
    " 'pct_delivery_behavior_devices_slope_T_2',\n",
    " 'pct_delivery_behavior_devices_slope_T_3',\n",
    " 'pct_delivery_behavior_devices_slope_T_4',\n",
    " 'pct_part_time_work_behavior_devices_baselined_T_1',\n",
    " 'pct_part_time_work_behavior_devices_baselined_T_2',\n",
    " 'pct_part_time_work_behavior_devices_baselined_T_3',\n",
    " 'pct_part_time_work_behavior_devices_baselined_T_4',\n",
    " 'pct_part_time_work_behavior_devices_slope_T_1',\n",
    " 'pct_part_time_work_behavior_devices_slope_T_2',\n",
    " 'pct_part_time_work_behavior_devices_slope_T_3',\n",
    " 'pct_part_time_work_behavior_devices_slope_T_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_full_features = list(safegraph_full_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_full_features = [i for i in safegraph_full_features if i not in features_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Removing inf values from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_df.replace([np.inf, -np.inf], np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols = covid_df.columns[covid_df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in na_cols:\n",
    "    covid_df[col] = covid_df.groupby(['date_start_period','STATEFP'])[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set training and testing size\n",
    "\n",
    "The dataset is initially divided into a 34-week subset for training and a 1-week subset for testing.\n",
    "\n",
    "At each time step, the size of training weeks increases by 1 and the test week has a 1-week shift towards the end of November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 30 # week\n",
    "testing_size = 1 # week\n",
    "num_counties = 3103\n",
    "time_steps = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training STXGB for one-week (7-day) prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_xgb = counties_shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2_xgb = dict()\n",
    "train_rmse_xgb = dict()\n",
    "train_mae_xgb = dict()\n",
    "test_rmse_xgb = dict()\n",
    "test_mae_xgb = dict()\n",
    "tuned_params_xgb = dict()\n",
    "\n",
    "\n",
    "\n",
    "models=['base', 'safegraph', 'facebook', 'safegraph_full']\n",
    "features = [base_features, safegraph_features, facebook_features, safegraph_full_features]\n",
    "\n",
    "# Setting Hyperparameters. Please refer to the SI for more information\n",
    "xgb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     gamma = np.arange(1,10,1),\n",
    "                     subsample = np.arange(0.1,0.5,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)]) \n",
    "\n",
    "\n",
    "\n",
    "for i in range(time_steps):\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "    \n",
    "    for model,feature in zip(models, features):\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #inititalization\n",
    "        xgb_model = xgboost.XGBRegressor(booster='gbtree', seed=42, verbosity=0) \n",
    "        \n",
    "        #cross validation\n",
    "        xgb_cv = RandomizedSearchCV(xgb_model, xgb_params, random_state=21, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        xgb_optimized = xgb_cv.fit(X_train, y_train)\n",
    "        best_xgb = xgb_optimized.best_estimator_\n",
    "        tuned_params_xgb[model, i] = xgb_optimized.best_params_\n",
    "\n",
    "        # model evaluation for training set\n",
    "        r2_train_xgb = round(best_xgb.score(X_train, y_train),2)\n",
    "        train_r2_xgb[model, i] = r2_train_xgb\n",
    "\n",
    "        y_train_predicted_xgb = best_xgb.predict(X_train)\n",
    "        rmse_train_xgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_xgb)))\n",
    "        train_rmse_xgb[model, i] = rmse_train_xgb\n",
    "        train_mae_xgb[model, i] =  mean_absolute_error(y_train, y_train_predicted_xgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_xgb = best_xgb.predict(X_test)\n",
    "        rmse_test_xgb = (np.sqrt(mean_squared_error(y_test, y_test_predicted_xgb)))\n",
    "        test_rmse_xgb[model, i] = rmse_test_xgb\n",
    "        test_mae_xgb[model, i] = mean_absolute_error(y_test, y_test_predicted_xgb)\n",
    "        \n",
    "        #feature importance\n",
    "        \n",
    "        xgb_importance = pd.concat([pd.DataFrame(feature, columns={'variable'}),\n",
    "                                    pd.DataFrame(np.transpose(best_xgb.feature_importances_), columns={'Importance'})],\n",
    "                                   axis = 1) \n",
    "        xgb_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        xgb_importance.to_csv(output_dir + 'XGB_importance_' + model + '_' + str(i) + '.csv')\n",
    "        \n",
    "        # add true values and predictions to the county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_xgb\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                        testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                        testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "        \n",
    "        test_cols = ['GEOID',  \n",
    "                      'y_test_'+ col_suffix, 'y_predicted_'+ col_suffix, \n",
    "                      'delta_inc_test_'+ col_suffix,  'delta_inc_pred_'+ col_suffix,\n",
    "                      'delta_case_test_'+ col_suffix, 'delta_case_pred_'+ col_suffix,\n",
    "                      'error_y_'+ col_suffix, 'error_delta_inc_'+ col_suffix, 'error_delta_case_'+ col_suffix]\n",
    "        \n",
    "        \n",
    "        counties_xgb = counties_xgb.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print('Training Model {} in time step {} finished in {} seconds!'.format(model, i, round(time_end-time_start, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output for performance analysis\n",
    "counties_xgb.to_file(output_dir + 'STXGB_1week_pred.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training STXGB for two-week (14-day) prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_xgb_14 = counties_shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2_xgb_14 = dict()\n",
    "train_rmse_xgb_14 = dict()\n",
    "train_mae_xgb_14 = dict()\n",
    "test_rmse_xgb_14 = dict()\n",
    "test_mae_xgb_14 = dict()\n",
    "tuned_params_xgb_14 = dict()\n",
    "\n",
    "\n",
    "\n",
    "models=['base', 'safegraph', 'facebook', 'safegraph_full']\n",
    "features = [base_features, safegraph_features, facebook_features, safegraph_full_features]\n",
    "\n",
    "# Setting Hyperparameters. Please refer to the SI for more information\n",
    "xgb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     gamma = np.arange(1,10,1),\n",
    "                     subsample = np.arange(0.1,0.5,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)]) \n",
    "\n",
    "\n",
    "\n",
    "for i in range(time_steps):\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "    \n",
    "    \n",
    "    for model,feature in zip(models, features):\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        # in the 2-week prediction model, the target variable is LOG_DELTA_INC_RATE_T_14\n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T_14']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T_14'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #inititalization\n",
    "        xgb_model = xgboost.XGBRegressor(booster='gbtree', seed=42, verbosity=0) \n",
    "        \n",
    "        #cross validation\n",
    "        xgb_cv = RandomizedSearchCV(xgb_model, xgb_params, random_state=21, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        \n",
    "        xgb_optimized = xgb_cv.fit(X_train, y_train)\n",
    "        best_xgb = xgb_optimized.best_estimator_\n",
    "        tuned_params_xgb_14[model, i] = xgb_optimized.best_params_\n",
    "\n",
    "        # model evaluation for training set\n",
    "        r2_train_xgb = round(best_xgb.score(X_train, y_train),2)\n",
    "        train_r2_xgb_14[model, i] = r2_train_xgb\n",
    "\n",
    "        y_train_predicted_xgb = best_xgb.predict(X_train)\n",
    "        rmse_train_xgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_xgb)))\n",
    "        train_rmse_xgb_14[model, i] = rmse_train_xgb\n",
    "        train_mae_xgb_14[model, i] =  mean_absolute_error(y_train, y_train_predicted_xgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_xgb = best_xgb.predict(X_test)\n",
    "        rmse_test_xgb = (np.sqrt(mean_squared_error(y_test, y_test_predicted_xgb)))\n",
    "        test_rmse_xgb_14[model, i] = rmse_test_xgb\n",
    "        test_mae_xgb_14[model, i] = mean_absolute_error(y_test, y_test_predicted_xgb)\n",
    "        \n",
    "        #feature importance\n",
    "        \n",
    "        xgb_importance = pd.concat([pd.DataFrame(feature, columns={'variable'}),\n",
    "                                    pd.DataFrame(np.transpose(best_xgb.feature_importances_), columns={'Importance'})],\n",
    "                                   axis = 1) \n",
    "        xgb_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        xgb_importance.to_csv(output_dir + 'XGB_importance_14_' + model + '_' + str(i) + '.csv')\n",
    "        \n",
    "        # add true values and predictions to the county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_xgb\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                      testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                      testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "        \n",
    "        test_cols = ['GEOID',  \n",
    "                      'y_test_'+ col_suffix, 'y_predicted_'+ col_suffix, \n",
    "                      'delta_inc_test_'+ col_suffix,  'delta_inc_pred_'+ col_suffix,\n",
    "                      'delta_case_test_'+ col_suffix, 'delta_case_pred_'+ col_suffix,\n",
    "                      'error_y_'+ col_suffix, 'error_delta_inc_'+ col_suffix, 'error_delta_case_'+ col_suffix]\n",
    "        \n",
    "        \n",
    "        counties_xgb_14 = counties_xgb_14.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print('Training Model {} in time step {} finished in {} seconds!'.format(model, i, round(time_end-time_start, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output for performance analysis\n",
    "counties_xgb_14.to_file(output_dir + 'STXGB_2week_pred.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training STXGB for three-week (21-day) prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2_xgb_21 = dict()\n",
    "train_rmse_xgb_21 = dict()\n",
    "train_mae_xgb_21 = dict()\n",
    "test_rmse_xgb_21 = dict()\n",
    "test_mae_xgb_21 = dict()\n",
    "tuned_params_xgb_21 = dict()\n",
    "\n",
    "\n",
    "\n",
    "models=['base', 'safegraph', 'facebook', 'safegraph_full']\n",
    "features = [base_features, safegraph_features, facebook_features, safegraph_full_features]\n",
    "\n",
    "# Setting Hyperparameters. Please refer to the SI for more information\n",
    "xgb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     gamma = np.arange(1,10,1),\n",
    "                     subsample = np.arange(0.1,0.5,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)]) \n",
    "\n",
    "\n",
    "\n",
    "for i in range(time_steps):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "    \n",
    "    \n",
    "    for model,feature in zip(models, features):\n",
    "        \n",
    "        # in the 3-week prediction model, the target variable is LOG_DELTA_INC_RATE_T_21\n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T_21']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T_21'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #inititalization\n",
    "        xgb_model = xgboost.XGBRegressor(booster='gbtree', seed=42, verbosity=0) \n",
    "        \n",
    "        #cross validation\n",
    "        xgb_cv = RandomizedSearchCV(xgb_model, xgb_params, random_state=21, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        xgb_optimized = xgb_cv.fit(X_train, y_train)\n",
    "        best_xgb = xgb_optimized.best_estimator_\n",
    "        tuned_params_xgb_21[model, i] = xgb_optimized.best_params_\n",
    "\n",
    "        # model evaluation for training set\n",
    "        r2_train_xgb = round(best_xgb.score(X_train, y_train),2)\n",
    "        train_r2_xgb_21[model, i] = r2_train_xgb\n",
    "\n",
    "        y_train_predicted_xgb = best_xgb.predict(X_train)\n",
    "        rmse_train_xgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_xgb)))\n",
    "        train_rmse_xgb_21[model, i] = rmse_train_xgb\n",
    "        train_mae_xgb_21[model, i] =  mean_absolute_error(y_train, y_train_predicted_xgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_xgb = best_xgb.predict(X_test)\n",
    "        rmse_test_xgb = (np.sqrt(mean_squared_error(y_test, y_test_predicted_xgb)))\n",
    "        test_rmse_xgb_21[model, i] = rmse_test_xgb\n",
    "        test_mae_xgb_21[model, i] = mean_absolute_error(y_test, y_test_predicted_xgb)\n",
    "        \n",
    "        #feature importance\n",
    "        \n",
    "        xgb_importance = pd.concat([pd.DataFrame(feature, columns={'variable'}),\n",
    "                                    pd.DataFrame(np.transpose(best_xgb.feature_importances_), columns={'Importance'})],\n",
    "                                   axis = 1) \n",
    "        xgb_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        xgb_importance.to_csv(output_dir + 'XGB_importance_21_' + model + '_' + str(i) + '.csv')\n",
    "        \n",
    "        # # add labels and predictions to a county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_xgb\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                    testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                      testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "        \n",
    "        test_cols = ['GEOID',  \n",
    "                      'y_test_'+ col_suffix, 'y_predicted_'+ col_suffix, \n",
    "                      'delta_inc_test_'+ col_suffix,  'delta_inc_pred_'+ col_suffix,\n",
    "                      'delta_case_test_'+ col_suffix, 'delta_case_pred_'+ col_suffix,\n",
    "                      'error_y_'+ col_suffix, 'error_delta_inc_'+ col_suffix, 'error_delta_case_'+ col_suffix]\n",
    "        \n",
    "        \n",
    "        counties_xgb_21 = counties_xgb_21.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        time_end = time.time()\n",
    "        \n",
    "        print('Training Model {} in time step {} finished in {} seconds!'.format(model, i, round(time_end-time_start, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output for performance analysis\n",
    "counties_xgb_21.to_file(output_dir + 'STXGB_3week_pred.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training STXGB for three-week (21-day) prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_xgb_28 = counties_shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2_xgb_28 = dict()\n",
    "train_rmse_xgb_28 = dict()\n",
    "train_mae_xgb_28 = dict()\n",
    "test_rmse_xgb_28 = dict()\n",
    "test_mae_xgb_28 = dict()\n",
    "tuned_params_xgb_28 = dict()\n",
    "\n",
    "\n",
    "\n",
    "models=['base', 'safegraph', 'facebook', 'safegraph_full']\n",
    "features = [base_features, safegraph_features, facebook_features, safegraph_full_features]\n",
    "\n",
    "\n",
    "xgb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     gamma = np.arange(1,10,1),\n",
    "                     subsample = np.arange(0.1,0.5,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)]) \n",
    "\n",
    "\n",
    "\n",
    "for i in range(time_steps):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "    \n",
    "    \n",
    "    for model,feature in zip(models, features):\n",
    "        \n",
    "        # in the 4-week prediction model, the target variable is LOG_DELTA_INC_RATE_T_28\n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T_28']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T_28'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #inititalization\n",
    "        xgb_model = xgboost.XGBRegressor(booster='gbtree', seed=42, verbosity=0) \n",
    "        \n",
    "        #cross validation\n",
    "        xgb_cv = RandomizedSearchCV(xgb_model, xgb_params, random_state=28, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        xgb_optimized = xgb_cv.fit(X_train, y_train)\n",
    "        best_xgb = xgb_optimized.best_estimator_\n",
    "        tuned_params_xgb_28[model, i] = xgb_optimized.best_params_\n",
    "\n",
    "        # model evaluation for training set\n",
    "        r2_train_xgb = round(best_xgb.score(X_train, y_train),2)\n",
    "        train_r2_xgb_28[model, i] = r2_train_xgb\n",
    "\n",
    "        y_train_predicted_xgb = best_xgb.predict(X_train)\n",
    "        rmse_train_xgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_xgb)))\n",
    "        train_rmse_xgb_28[model, i] = rmse_train_xgb\n",
    "        train_mae_xgb_28[model, i] =  mean_absolute_error(y_train, y_train_predicted_xgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_xgb = best_xgb.predict(X_test)\n",
    "        rmse_test_xgb = (np.sqrt(mean_squared_error(y_test, y_test_predicted_xgb)))\n",
    "        test_rmse_xgb_28[model, i] = rmse_test_xgb\n",
    "        test_mae_xgb_28[model, i] = mean_absolute_error(y_test, y_test_predicted_xgb)\n",
    "        \n",
    "        #feature importance\n",
    "        \n",
    "        xgb_importance = pd.concat([pd.DataFrame(feature, columns={'variable'}),\n",
    "                                    pd.DataFrame(np.transpose(best_xgb.feature_importances_), columns={'Importance'})],\n",
    "                                   axis = 1) \n",
    "        xgb_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        xgb_importance.to_csv(output_dir + 'XGB_importance_28_' + model + '_' + str(i) + '.csv')\n",
    "        \n",
    "        # # add labels and predictions to a county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_xgb\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                      testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                      testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "        \n",
    "        test_cols = ['GEOID',  \n",
    "                      'y_test_'+ col_suffix, 'y_predicted_'+ col_suffix, \n",
    "                      'delta_inc_test_'+ col_suffix,  'delta_inc_pred_'+ col_suffix,\n",
    "                      'delta_case_test_'+ col_suffix, 'delta_case_pred_'+ col_suffix,\n",
    "                      'error_y_'+ col_suffix, 'error_delta_inc_'+ col_suffix, 'error_delta_case_'+ col_suffix]\n",
    "        \n",
    "        \n",
    "        counties_xgb_28 = counties_xgb_28.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        time_end = time.time()\n",
    "        \n",
    "        print('Training Model {} in time step {} finished in {} seconds!'.format(model, i, round(time_end-time_start, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output for performance analysis\n",
    "counties_xgb_28.to_file(output_dir + 'STXGB_4week_pred.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving GeoJson files for error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating error maps and csvs for mapping and plotting STXGB models\n",
    "\n",
    "\n",
    "xgb_error_shapefiles = [counties_xgb, counties_xgb_14, counties_xgb_21, counties_xgb_28]\n",
    "xgb_error_names = ['XGB_defalt', 'XGB_14', 'XGB_21', 'XGB_28']\n",
    "\n",
    "\n",
    "\n",
    "for error_shp, error_name in zip(xgb_error_shapefiles, xgb_error_names):\n",
    "    \n",
    "    models=['base', 'safegraph', 'facebook', 'safegraph_full']\n",
    "    \n",
    "    \n",
    "    for model in models:\n",
    "\n",
    "        model_cols = [col for col in y_error if model in col]\n",
    "        xgb_errors['error_y_' + model + '_avg'] = xgb_errors[model_cols].mean(axis=1)\n",
    "\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        model_cols = [col for col in delta_inc_error if model in col]\n",
    "        xgb_errors['error_delta_inc_' + model + '_avg'] = xgb_errors[model_cols].mean(axis=1)\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        model_cols = [col for col in delta_case_error if model in col]\n",
    "        xgb_errors['error_delta_case_' + model + '_avg'] = xgb_errors[model_cols].mean(axis=1)\n",
    "\n",
    "    xgb_errors.to_file(output_dir + 'error_map_' + error_name + '.geojson', driver='GeoJSON')\n",
    "    \n",
    "    #Save Maps\n",
    "    error_df_xgb = pd.DataFrame(index=index_temp, columns=['RMSE case', 'MAE case', 'RMSE inc', 'MAE inc'], dtype=float)\n",
    "\n",
    "\n",
    "    for model in models:\n",
    "        for i in range(time_steps):\n",
    "            rmse_case = np.sqrt(mean_squared_error(xgb_errors['delta_case_test_' + model + '_' + str(i)],\n",
    "                                               xgb_errors['delta_case_pred_' + model + '_' + str(i)]))\n",
    "            rmse_inc = np.sqrt(mean_squared_error(xgb_errors['delta_inc_test_' + model + '_' + str(i)],\n",
    "                                               xgb_errors['delta_inc_pred_' + model + '_' + str(i)]))\n",
    "\n",
    "            mae_case = mean_absolute_error(xgb_errors['delta_case_test_' + model + '_' + str(i)],\n",
    "                                               xgb_errors['delta_case_pred_' + model + '_' + str(i)])\n",
    "            mae_inc = mean_absolute_error(xgb_errors['delta_inc_test_' + model + '_' + str(i)],\n",
    "                                               xgb_errors['delta_inc_pred_' + model + '_' + str(i)])\n",
    "\n",
    "            error_df_xgb.loc[model, i] = [rmse_case, mae_case, rmse_inc, mae_inc]\n",
    "            \n",
    "    error_df_xgb.to_csv(output_dir + 'error_' + error_name + '.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FPC and SPC regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
