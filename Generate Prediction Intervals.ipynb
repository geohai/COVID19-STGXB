{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Intervals\n",
    "\n",
    "This notebook is used to generate 95% prediction intervals. The XGBoost algorithm does not support interval prediction, therefore we have used Stochastic Gradient Boosting Regressor for generating interval predictions.\n",
    "\n",
    "\n",
    "For each time step, we train 3 models. One for point predictions (which is only used for comparison with XGB predictions), one for lower quantile prediction, and one for upper quantile prediction. The latter two use a `quantile` loss (`alpha`= 0.025 and 0.975 respectively) whereas the first one uses `neg_root_mean_squared_error`.\n",
    "\n",
    "\n",
    "ections 1 and 2 of this notebook is very similar to the main notebook (`STXGB model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO4dO2DGcoM8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from scipy.stats import norm\n",
    "import geopandas as gpd\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SET ALPHA VALUE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output directory\n",
    "# You can change it if you want to\n",
    "output_dir = './output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIirtJ6Ojnrk"
   },
   "source": [
    "## 1- Loading Data\n",
    "\n",
    "The `all_features_v1.csv` contains all of the features and target variables that we have used in different variants of STXGB model (STXGB-FB, STXGB-SG, and STXGB-SGR) and for 1- to 4-week prediction horizons. \n",
    "\n",
    "\n",
    "This file is published publicly alongside the code, so you can download the csv file and run STXGB models for yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load data from Zenodo URL\n",
    "\n",
    "df_url = 'https://zenodo.org/record/5533027/files/all_features_v1_0.csv?download=1'\n",
    "covid_df = pd.read_csv(df_url,index_col=0, dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a base geojson file \n",
    "\n",
    "This file contains county FIPS and is used to store model outputs is a georeferenced format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://drive.google.com/file/d/1MVyLzzHl3hzno4o1rLZtI0peqIi23zsr/view?usp=sharing'\n",
    "url_counties='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "counties_shp = gpd.read_file(url_counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The list of features in the STXGB-FB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cols = [col for col in covid_df.columns if 'TEMP' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_cols = ['POP_DENSITY',\n",
    "'PCT_MALE',\n",
    "'PCT_65_OVE',\n",
    "'PCT_BLACK',\n",
    "'PCT_HISPAN', \n",
    "'PCT_AMIND',\n",
    "'PCT_RURAL',\n",
    "'PCT_COL_DE' ,\n",
    "'PCT_TRUMP_',\n",
    "'MED_HOS_IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_cols = [col for col in covid_df.columns if 'DELTA_INC' in col]\n",
    "inc_cols.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_cols = [col for col in covid_df.columns if 'DELTA_SPC_T' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_cols = [col for col in covid_df.columns if 'REL_' in col]\n",
    "rel_cols_non_delta = [col for col in rel_cols if 'DELTA' in col]\n",
    "rel_cols = list(set(rel_cols)^set(rel_cols_non_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_cols = [col for col in covid_df.columns if 'RATIO_' in col]\n",
    "ratio_cols_non_delta = [col for col in ratio_cols if 'DELTA' in col]\n",
    "ratio_cols = list(set(ratio_cols)^set(ratio_cols_non_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_features = socio_cols + temp_cols + rel_cols + ratio_cols + spc_cols  + inc_cols  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_features.extend(('LOG_MEAN_INC_RATE_T_4', 'MEAN_SPC_T_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. The list of features in the STXGB-SG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpc_cols = [col for col in covid_df.columns if 'DELTA_FPC_T' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_home_cols = [col for col in covid_df.columns if 'completely_home_' in col]\n",
    "pct_home_cols_non_base = [col for col in pct_home_cols if 'baselined' in col]\n",
    "pct_home_cols = list(set(pct_home_cols)^set(pct_home_cols_non_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_traveled_cols = [col for col in covid_df.columns if 'distance_traveled_' in col]\n",
    "dist_traveled_cols_non_current = [col for col in dist_traveled_cols if 'current' in col]\n",
    "dist_traveled_cols = list(set(dist_traveled_cols)^set(dist_traveled_cols_non_current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_features = socio_cols + temp_cols + pct_home_cols + dist_traveled_cols + \\\n",
    "                     fpc_cols + inc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_features.extend(('LOG_MEAN_INC_RATE_T_4','MEAN_FPC_T_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set training and testing size\n",
    "\n",
    "The dataset is initially divided into a 34-week subset for training and a 1-week subset for testing.\n",
    "\n",
    "At each time step, the size of training weeks increases by 1 and the test week has a 1-week shift towards the end of November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 30 # week\n",
    "testing_size = 1 # week\n",
    "num_counties = 3103\n",
    "time_steps = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate PIs for one-week (7-day) prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_7 = counties_shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2 = dict()\n",
    "train_rmse = dict()\n",
    "train_mae = dict()\n",
    "test_rmse = dict()\n",
    "test_mae = dict()\n",
    "tuned_params_sgb_7 = dict()\n",
    "tuned_params_sgb_lower_7 = dict()\n",
    "tuned_params_sgb_upper_7 = dict()\n",
    "\n",
    "models=['safegraph', 'facebook']\n",
    "features = [safegraph_features, facebook_features]\n",
    "\n",
    "gb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     subsample = np.arange(0.1,0.9,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)],\n",
    "                     max_features = ['sqrt', 'log2']) \n",
    "\n",
    "for i in range(time_steps):\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "\n",
    "    for model,feature in zip(models, features):\n",
    "    \n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #cross validation\n",
    "        sgb_regressor_point = GradientBoostingRegressor(random_state=7)\n",
    "\n",
    "        sgb_cv = RandomizedSearchCV(sgb_regressor_point, gb_params, random_state=7, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        \n",
    "        sgb_optimized = sgb_cv.fit(X_train, y_train)\n",
    "        best_sgb = sgb_optimized.best_estimator_\n",
    "        tuned_params_sgb_7[model, i] = sgb_optimized.best_params_\n",
    "        \n",
    "        \n",
    "        # model evaluation for training set\n",
    "        train_r2_sgb = round(best_sgb.score(X_train, y_train),2)\n",
    "        train_r2[model, i] = train_r2_sgb\n",
    "\n",
    "        y_train_predicted_sgb = best_sgb.predict(X_train)\n",
    "        rmse_train_sgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_sgb)))\n",
    "        train_rmse[model, i] = rmse_train_sgb\n",
    "        train_mae[model, i] =  mean_absolute_error(y_train, y_train_predicted_sgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_sgb = best_sgb.predict(X_test)\n",
    "        rmse_test_gbr = (np.sqrt(mean_squared_error(y_test, y_test_predicted_sgb)))\n",
    "        test_rmse[model, i] = rmse_test_gbr\n",
    "        test_mae[model, i] = mean_absolute_error(y_test, y_test_predicted_sgb)\n",
    "        \n",
    "        \n",
    "        # lower and upper interval predictions\n",
    "        sgb_regressor_lower = best_sgb.set_params(loss='quantile', alpha=1-alpha)\n",
    "        sgb_regressor_lower = sgb_regressor_lower.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predicted_sgb_lower = sgb_regressor_lower.predict(X_test)\n",
    "        \n",
    "        sgb_regressor_upper = best_sgb.set_params(loss='quantile', alpha=alpha)\n",
    "        sgb_regressor_upper = sgb_regressor_upper.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        y_test_predicted_sgb_upper = sgb_regressor_upper.predict(X_test)\n",
    "        \n",
    "        \n",
    "        # add labels and predictions to a county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_sgb\n",
    "        testing_df.loc[:,'y_predicted_lower_'+ col_suffix] = y_test_predicted_sgb_lower\n",
    "        testing_df.loc[:,'y_predicted_upper_'+ col_suffix] = y_test_predicted_sgb_upper\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_lower_'+ col_suffix] = np.exp(testing_df['y_predicted_lower_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_upper_'+ col_suffix] = np.exp(testing_df['y_predicted_upper_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                          testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                          testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_lower_'+ col_suffix] = (testing_df['delta_inc_pred_lower_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_upper_'+ col_suffix] = (testing_df['delta_inc_pred_upper_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "\n",
    "        \n",
    "        counties_sgb_interval_7 = counties_sgb_interval_7.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        print('Model {} in time step {} done!'.format(model, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_7.to_file(output_dir + 'counties_sgb_interval_7.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate PIs for two-week prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_14 = counties_shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2 = dict()\n",
    "train_rmse = dict()\n",
    "train_mae = dict()\n",
    "test_rmse = dict()\n",
    "test_mae = dict()\n",
    "tuned_params_sgb_14 = dict()\n",
    "tuned_params_sgb_lower_14 = dict()\n",
    "tuned_params_sgb_upper_14 = dict()\n",
    "\n",
    "models=['safegraph', 'facebook']\n",
    "features = [safegraph_features, facebook_features]\n",
    "\n",
    "gb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     subsample = np.arange(0.1,0.9,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)],\n",
    "                     max_features = ['sqrt', 'log2']) \n",
    "\n",
    "for i in range(time_steps):\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "    \n",
    "    for model,feature in zip(models, features):\n",
    "    \n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T_14']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T_14'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #cross validation\n",
    "        sgb_regressor_point = GradientBoostingRegressor(random_state=14)\n",
    "\n",
    "        sgb_cv = RandomizedSearchCV(sgb_regressor_point, gb_params, random_state=14, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        sgb_optimized = sgb_cv.fit(X_train, y_train)\n",
    "        best_sgb = sgb_optimized.best_estimator_\n",
    "        tuned_params_sgb_14[model, i] = sgb_optimized.best_params_\n",
    "        \n",
    "        \n",
    "        # model evaluation for training set\n",
    "        train_r2_sgb = round(best_sgb.score(X_train, y_train),2)\n",
    "        train_r2[model, i] = train_r2_sgb\n",
    "\n",
    "        y_train_predicted_sgb = best_sgb.predict(X_train)\n",
    "        rmse_train_sgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_sgb)))\n",
    "        train_rmse[model, i] = rmse_train_sgb\n",
    "        train_mae[model, i] =  mean_absolute_error(y_train, y_train_predicted_sgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_sgb = best_sgb.predict(X_test)\n",
    "        rmse_test_gbr = (np.sqrt(mean_squared_error(y_test, y_test_predicted_sgb)))\n",
    "        test_rmse[model, i] = rmse_test_gbr\n",
    "        test_mae[model, i] = mean_absolute_error(y_test, y_test_predicted_sgb)\n",
    "        \n",
    "        \n",
    "        # lower and upper interval predictions\n",
    "        sgb_regressor_lower = best_sgb.set_params(loss='quantile', alpha=1-alpha)\n",
    "        sgb_regressor_lower = sgb_regressor_lower.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predicted_sgb_lower = sgb_regressor_lower.predict(X_test)\n",
    "        \n",
    "        sgb_regressor_upper = best_sgb.set_params(loss='quantile', alpha=alpha)\n",
    "        sgb_regressor_upper = sgb_regressor_upper.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        y_test_predicted_sgb_upper = sgb_regressor_upper.predict(X_test)\n",
    "    \n",
    "\n",
    "        \n",
    "        # add labels and predictions to a county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_sgb\n",
    "        testing_df.loc[:,'y_predicted_lower_'+ col_suffix] = y_test_predicted_sgb_lower\n",
    "        testing_df.loc[:,'y_predicted_upper_'+ col_suffix] = y_test_predicted_sgb_upper\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_lower_'+ col_suffix] = np.exp(testing_df['y_predicted_lower_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_upper_'+ col_suffix] = np.exp(testing_df['y_predicted_upper_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                      testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                      testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_lower_'+ col_suffix] = (testing_df['delta_inc_pred_lower_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_upper_'+ col_suffix] = (testing_df['delta_inc_pred_upper_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - \\\n",
    "                                                testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "        \n",
    "        test_cols = ['GEOID',  \n",
    "                     'y_test_'+ col_suffix, 'y_predicted_'+ col_suffix, \n",
    "                     'delta_inc_test_'+ col_suffix,  'delta_inc_pred_'+ col_suffix,\n",
    "                     'delta_case_test_'+ col_suffix, 'delta_case_pred_'+ col_suffix,\n",
    "                     'error_y_'+ col_suffix, 'error_delta_inc_'+ col_suffix, 'error_delta_case_'+ col_suffix,\n",
    "                    'y_predicted_lower_'+ col_suffix, 'y_predicted_upper_'+ col_suffix,\n",
    "                    'delta_case_pred_lower_'+ col_suffix, 'delta_case_pred_upper_'+ col_suffix]\n",
    "\n",
    "        \n",
    "        counties_sgb_interval_14 = counties_sgb_interval_14.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        print('Model {} in time step {} done!'.format(model, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_14.to_file(output_dir + 'counties_sgb_interval_14.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate PIs for three-week prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_21 = counties_shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2 = dict()\n",
    "train_rmse = dict()\n",
    "train_mae = dict()\n",
    "test_rmse = dict()\n",
    "test_mae = dict()\n",
    "tuned_params_sgb_21 = dict()\n",
    "tuned_params_sgb_lower_21 = dict()\n",
    "tuned_params_sgb_upper_21 = dict()\n",
    "\n",
    "models=['safegraph', 'facebook']\n",
    "features = [safegraph_features, facebook_features]\n",
    "\n",
    "gb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     subsample = np.arange(0.1,0.9,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)],\n",
    "                     max_features = ['sqrt', 'log2']) \n",
    "\n",
    "for i in range(time_steps):\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "\n",
    "    \n",
    "    for model,feature in zip(models, features):\n",
    "    \n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T_21']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T_21'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #cross validation\n",
    "        sgb_regressor_point = GradientBoostingRegressor(random_state=21)\n",
    "\n",
    "        sgb_cv = RandomizedSearchCV(sgb_regressor_point, gb_params, random_state=21, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        \n",
    "        sgb_optimized = sgb_cv.fit(X_train, y_train)\n",
    "        best_sgb = sgb_optimized.best_estimator_\n",
    "        tuned_params_sgb_21[model, i] = sgb_optimized.best_params_\n",
    "        \n",
    "        \n",
    "        # model evaluation for training set\n",
    "        train_r2_sgb = round(best_sgb.score(X_train, y_train),2)\n",
    "        train_r2[model, i] = train_r2_sgb\n",
    "\n",
    "        y_train_predicted_sgb = best_sgb.predict(X_train)\n",
    "        rmse_train_sgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_sgb)))\n",
    "        train_rmse[model, i] = rmse_train_sgb\n",
    "        train_mae[model, i] =  mean_absolute_error(y_train, y_train_predicted_sgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_sgb = best_sgb.predict(X_test)\n",
    "        rmse_test_gbr = (np.sqrt(mean_squared_error(y_test, y_test_predicted_sgb)))\n",
    "        test_rmse[model, i] = rmse_test_gbr\n",
    "        test_mae[model, i] = mean_absolute_error(y_test, y_test_predicted_sgb)\n",
    "        \n",
    "        \n",
    "        # lower and upper interval predictions\n",
    "        sgb_regressor_lower = best_sgb.set_params(loss='quantile', alpha=1-alpha)\n",
    "        sgb_regressor_lower = sgb_regressor_lower.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predicted_sgb_lower = sgb_regressor_lower.predict(X_test)\n",
    "        \n",
    "        sgb_regressor_upper = best_sgb.set_params(loss='quantile', alpha=alpha)\n",
    "        sgb_regressor_upper = sgb_regressor_upper.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        y_test_predicted_sgb_upper = sgb_regressor_upper.predict(X_test)\n",
    "        \n",
    "        \n",
    "        # add labels and predictions to a county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_sgb\n",
    "        testing_df.loc[:,'y_predicted_lower_'+ col_suffix] = y_test_predicted_sgb_lower\n",
    "        testing_df.loc[:,'y_predicted_upper_'+ col_suffix] = y_test_predicted_sgb_upper\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_lower_'+ col_suffix] = np.exp(testing_df['y_predicted_lower_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_upper_'+ col_suffix] = np.exp(testing_df['y_predicted_upper_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                          testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                          testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_lower_'+ col_suffix] = (testing_df['delta_inc_pred_lower_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_upper_'+ col_suffix] = (testing_df['delta_inc_pred_upper_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "        \n",
    "        test_cols = ['GEOID',  \n",
    "                     'y_test_'+ col_suffix, 'y_predicted_'+ col_suffix, \n",
    "                     'delta_inc_test_'+ col_suffix,  'delta_inc_pred_'+ col_suffix,\n",
    "                     'delta_case_test_'+ col_suffix, 'delta_case_pred_'+ col_suffix,\n",
    "                     'error_y_'+ col_suffix, 'error_delta_inc_'+ col_suffix, 'error_delta_case_'+ col_suffix,\n",
    "                    'y_predicted_lower_'+ col_suffix, 'y_predicted_upper_'+ col_suffix,\n",
    "                    'delta_case_pred_lower_'+ col_suffix, 'delta_case_pred_upper_'+ col_suffix]\n",
    "\n",
    "        \n",
    "        counties_sgb_interval_21 = counties_sgb_interval_21.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        print('Model {} in time step {} done!'.format(model, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_21.to_file(output_dir + 'counties_sgb_interval_21.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate PIs for four-week prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_28 = counties_shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2 = dict()\n",
    "train_rmse = dict()\n",
    "train_mae = dict()\n",
    "test_rmse = dict()\n",
    "test_mae = dict()\n",
    "tuned_params_sgb_28 = dict()\n",
    "tuned_params_sgb_lower_28 = dict()\n",
    "tuned_params_sgb_upper_28 = dict()\n",
    "\n",
    "models=['safegraph', 'facebook']\n",
    "features = [safegraph_features, facebook_features]\n",
    "\n",
    "gb_params = dict(learning_rate=np.arange(0.05,0.3,0.05), \n",
    "                     n_estimators=np.arange(100,1000,100), \n",
    "                     subsample = np.arange(0.1,0.9,0.05),\n",
    "                     max_depth=[int(i) for i in np.arange(1,10,1)],\n",
    "                     max_features = ['sqrt', 'log2']) \n",
    "\n",
    "for i in range(time_steps):\n",
    "    \n",
    "    training_df = covid_df.iloc[:(i+training_size)*num_counties,:]\n",
    "    testing_df = covid_df.iloc[(i+training_size)*num_counties:(i+training_size+testing_size)*num_counties,:]\n",
    "    \n",
    "    for model,feature in zip(models, features):\n",
    "    \n",
    "        X_train = training_df[feature]\n",
    "        y_train = training_df['LOG_DELTA_INC_RATE_T_28']\n",
    "        X_test = testing_df[feature]\n",
    "        y_test = testing_df['LOG_DELTA_INC_RATE_T_28'] \n",
    "\n",
    "        #scaling X\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        #cross validation\n",
    "        sgb_regressor_point = GradientBoostingRegressor(random_state=28)\n",
    "\n",
    "        sgb_cv = RandomizedSearchCV(sgb_regressor_point, gb_params, random_state=28, \n",
    "                                    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        sgb_optimized = sgb_cv.fit(X_train, y_train)\n",
    "        best_sgb = sgb_optimized.best_estimator_\n",
    "        tuned_params_sgb_28[model, i] = sgb_optimized.best_params_\n",
    "        \n",
    "        \n",
    "        # model evaluation for training set\n",
    "        train_r2_sgb = round(best_sgb.score(X_train, y_train),2)\n",
    "        train_r2[model, i] = train_r2_sgb\n",
    "\n",
    "        y_train_predicted_sgb = best_sgb.predict(X_train)\n",
    "        rmse_train_sgb = (np.sqrt(mean_squared_error(y_train, y_train_predicted_sgb)))\n",
    "        train_rmse[model, i] = rmse_train_sgb\n",
    "        train_mae[model, i] =  mean_absolute_error(y_train, y_train_predicted_sgb)\n",
    "\n",
    "        # model evaluation for test set\n",
    "        y_test_predicted_sgb = best_sgb.predict(X_test)\n",
    "        rmse_test_gbr = (np.sqrt(mean_squared_error(y_test, y_test_predicted_sgb)))\n",
    "        test_rmse[model, i] = rmse_test_gbr\n",
    "        test_mae[model, i] = mean_absolute_error(y_test, y_test_predicted_sgb)\n",
    "        \n",
    "        \n",
    "        # lower and upper interval predictions\n",
    "        sgb_regressor_lower = best_sgb.set_params(loss='quantile', alpha=1-alpha)\n",
    "        sgb_regressor_lower = sgb_regressor_lower.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predicted_sgb_lower = sgb_regressor_lower.predict(X_test)\n",
    "        \n",
    "        sgb_regressor_upper = best_sgb.set_params(loss='quantile', alpha=alpha)\n",
    "        sgb_regressor_upper = sgb_regressor_upper.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        y_test_predicted_sgb_upper = sgb_regressor_upper.predict(X_test)\n",
    "\n",
    "        \n",
    "        # add labels and predictions to a county data frame\n",
    "        col_suffix = model +'_' + str(i)\n",
    "        \n",
    "        testing_df.loc[:,'y_test_'+ col_suffix] = y_test\n",
    "        testing_df.loc[:,'y_predicted_'+ col_suffix] = y_test_predicted_sgb\n",
    "        testing_df.loc[:,'y_predicted_lower_'+ col_suffix] = y_test_predicted_sgb_lower\n",
    "        testing_df.loc[:,'y_predicted_upper_'+ col_suffix] = y_test_predicted_sgb_upper\n",
    "        \n",
    "        testing_df['delta_inc_test_'+ col_suffix] = np.exp(testing_df['y_test_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_'+ col_suffix] = np.exp(testing_df['y_predicted_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_lower_'+ col_suffix] = np.exp(testing_df['y_predicted_lower_'+ col_suffix]) - 1\n",
    "        testing_df['delta_inc_pred_upper_'+ col_suffix] = np.exp(testing_df['y_predicted_upper_'+ col_suffix]) - 1\n",
    "        \n",
    "        testing_df['delta_case_test_'+ col_suffix] = (testing_df['delta_inc_test_'+ col_suffix] * \n",
    "                                                          testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_'+ col_suffix] = (testing_df['delta_inc_pred_'+ col_suffix] * \n",
    "                                                          testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_lower_'+ col_suffix] = (testing_df['delta_inc_pred_lower_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['delta_case_pred_upper_'+ col_suffix] = (testing_df['delta_inc_pred_upper_'+ col_suffix] * \n",
    "                                                            testing_df['POPULATION']) / 10000\n",
    "        \n",
    "        testing_df['error_y_'+ col_suffix] = testing_df['y_test_'+ col_suffix] - testing_df['y_predicted_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_inc_'+ col_suffix] = testing_df['delta_inc_test_'+ col_suffix] - \\\n",
    "                                                        testing_df['delta_inc_pred_'+ col_suffix]\n",
    "        \n",
    "        testing_df['error_delta_case_'+ col_suffix] = testing_df['delta_case_test_'+ col_suffix] - \\ \n",
    "                                                        testing_df['delta_case_pred_'+ col_suffix]\n",
    "\n",
    "        \n",
    "        counties_sgb_interval_28 = counties_sgb_interval_28.merge(testing_df[test_cols], how='left', on='GEOID')\n",
    "        \n",
    "        print('Model {} in time step {} done!'.format(model, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_sgb_interval_28.to_file(output_dir + 'counties_sgb_interval_28.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FPC and SPC regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
