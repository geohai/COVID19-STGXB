{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This notebook is used to clean up, preprocess, and aggregate data necessary for STXGB models.\n",
    "\n",
    "Please refer to the `Method` section of the article and Supplementary Information document for more information.\n",
    "\n",
    "\n",
    "In order to run this notebook, you have to download the required datasets from this [address](https://drive.google.com/drive/u/1/folders/1laAZFCvsPLLaKDvg0isTMMr20kMe0x_r). Once downloaded, set the directory in which you have save the files as `input_directory` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = './input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing global # gdal: DRIVER_NAME declaration in C:\\Users\\BehzadVahediTorghabe\\anaconda3\\Lib\\site-packages\\osgeo\\gdal_array.py\n",
      "Missing global # gdal: DRIVER_NAME declaration in C:\\Users\\BehzadVahediTorghabe\\anaconda3\\Lib\\site-packages\\osgeo\\gdal_array.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set date string formatting based on operating system\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    conversion_format = '%#m/%#d/%y'\n",
    "else:\n",
    "    conversion_format = '%-m/%-d/%y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_file( input_directory + 'Contiguous_US.geojson')\n",
    "\n",
    "# Or alternatively:\n",
    "# url='https://drive.google.com/file/d/1MVyLzzHl3hzno4o1rLZtI0peqIi23zsr/view?usp=sharing'\n",
    "# url_counties='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "# data = gpd.read_file(url_counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['STATEFP'] = data.apply(lambda L: L.GEOID[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global number_counties \n",
    "number_counties = data.shape[0] #3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(by='GEOID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COVID data and apply smoothing \n",
    "\n",
    "To alleviate inconsistencies in reporting COVID-19 cases, we apply a 7-day moving average to the case data published by JHU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_JH_covid_data(target, smooth):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    --------------\n",
    "        target: str\n",
    "            the target variable: either 'case' or 'death'\n",
    "            \n",
    "        smooth: bool\n",
    "            wether to smooth the data frame or not.\n",
    "            The smoothing is done by using a 7-day rolling average   \n",
    "    '''\n",
    "    \n",
    "    assert isinstance(smooth, bool), \"Smooth must be a boolean variable!\"\n",
    "    \n",
    "    base_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/'\n",
    "    \n",
    "    \n",
    "    if target == 'case':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_']\n",
    "\n",
    "    elif target=='death':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_','Population']\n",
    "    else:\n",
    "        print(\"invalid argument for target. Acceptable values are: 'case' or 'death'\")\n",
    "        return None\n",
    "\n",
    "    jh_covid_df = pd.read_csv(jh_data_url)\n",
    "\n",
    "    # preprocessing JH COVID data\n",
    "    jh_covid_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    jh_covid_df['FIPS'] = jh_covid_df['FIPS'].astype('int64')\n",
    "\n",
    "    jh_covid_df.drop(columns=cols_to_drp, inplace=True)\n",
    "\n",
    "    #Important: check to see the column index is adherent to the imported df\n",
    "\n",
    "    first_date = datetime.strptime(jh_covid_df.columns[4], '%m/%d/%y').date()\n",
    "\n",
    "    last_date = datetime.strptime(jh_covid_df.columns[-1], '%m/%d/%y').date()\n",
    "\n",
    "\n",
    "    current_date = last_date\n",
    "\n",
    "    previous_date = last_date - timedelta (days=1)\n",
    "\n",
    "\n",
    "    while current_date > first_date:\n",
    "\n",
    "        #For unix, replace # with - in the time format\n",
    "\n",
    "        current_col = current_date.strftime(conversion_format) #replace # with - in Mac or Linux\n",
    "\n",
    "        previous_col = previous_date.strftime(conversion_format)\n",
    "\n",
    "        jh_covid_df[previous_col] = np.where(jh_covid_df[previous_col] > jh_covid_df[current_col], \n",
    "                                             jh_covid_df[current_col], jh_covid_df[previous_col])\n",
    "\n",
    "        current_date = current_date - timedelta(days=1)\n",
    "\n",
    "        previous_date = previous_date - timedelta(days=1)\n",
    "        \n",
    "    \n",
    "    if smooth:\n",
    "        jh_covid_df.iloc[:,4:] = jh_covid_df.iloc[:,4:].rolling(7,min_periods=1,axis=1).mean()\n",
    "\n",
    "\n",
    "    return jh_covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = get_JH_covid_data('case', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Facebook Movement Range Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility = pd.read_csv(input_directory + 'movement-range-2021-03-02.txt', sep=\"\\t\", dtype={'polygon_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us = fb_mobility[fb_mobility['country']=='USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique counties for which we have at least one day of data\n",
    "len(fb_mobility_us['polygon_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting Counties in the contiguous US for which there is no data in FB mobility\n",
    "contiguous_fips = set(data['GEOID']) # number of unique fips: 3103\n",
    "mobility_fips = set(fb_mobility_us['polygon_id']) # number of unique fips: 2694\n",
    "i = 0\n",
    "missing_fips = []\n",
    "for fips in contiguous_fips:\n",
    "    if (fips in mobility_fips):\n",
    "        i+=1\n",
    "    else:\n",
    "        missing_fips.append(fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Counties in the contiguous US for which there is no data in FB mobility\n",
    "len(missing_fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe as transpose of the above, with days as columns and counties as rows\n",
    "\n",
    "relative_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)\n",
    "ratio_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>country</th>\n",
       "      <th>polygon_source</th>\n",
       "      <th>polygon_id</th>\n",
       "      <th>polygon_name</th>\n",
       "      <th>all_day_bing_tiles_visited_relative_change</th>\n",
       "      <th>all_day_ratio_single_tile_users</th>\n",
       "      <th>baseline_name</th>\n",
       "      <th>baseline_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4742224</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.04845</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742225</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.03275</td>\n",
       "      <td>0.15003</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742226</th>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.05122</td>\n",
       "      <td>0.14391</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742227</th>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>-0.04741</td>\n",
       "      <td>0.16058</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742228</th>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>01001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.06285</td>\n",
       "      <td>0.15298</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648079</th>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>78030</td>\n",
       "      <td>St. Thomas</td>\n",
       "      <td>-0.00104</td>\n",
       "      <td>0.12959</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648080</th>\n",
       "      <td>2021-02-27</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>78030</td>\n",
       "      <td>St. Thomas</td>\n",
       "      <td>-0.05528</td>\n",
       "      <td>0.16893</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648081</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>78030</td>\n",
       "      <td>St. Thomas</td>\n",
       "      <td>-0.10939</td>\n",
       "      <td>0.21618</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648082</th>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>78030</td>\n",
       "      <td>St. Thomas</td>\n",
       "      <td>-0.13259</td>\n",
       "      <td>0.16558</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648083</th>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>USA</td>\n",
       "      <td>FIPS</td>\n",
       "      <td>78030</td>\n",
       "      <td>St. Thomas</td>\n",
       "      <td>-0.07999</td>\n",
       "      <td>0.14719</td>\n",
       "      <td>full_february_except_presidents_day</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>905860 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ds country polygon_source polygon_id polygon_name  \\\n",
       "4742224  2020-03-01     USA           FIPS      01001      Autauga   \n",
       "4742225  2020-03-02     USA           FIPS      01001      Autauga   \n",
       "4742226  2020-03-03     USA           FIPS      01001      Autauga   \n",
       "4742227  2020-03-04     USA           FIPS      01001      Autauga   \n",
       "4742228  2020-03-05     USA           FIPS      01001      Autauga   \n",
       "...             ...     ...            ...        ...          ...   \n",
       "5648079  2021-02-26     USA           FIPS      78030   St. Thomas   \n",
       "5648080  2021-02-27     USA           FIPS      78030   St. Thomas   \n",
       "5648081  2021-02-28     USA           FIPS      78030   St. Thomas   \n",
       "5648082  2021-03-01     USA           FIPS      78030   St. Thomas   \n",
       "5648083  2021-03-02     USA           FIPS      78030   St. Thomas   \n",
       "\n",
       "         all_day_bing_tiles_visited_relative_change  \\\n",
       "4742224                                     0.04845   \n",
       "4742225                                     0.03275   \n",
       "4742226                                     0.05122   \n",
       "4742227                                    -0.04741   \n",
       "4742228                                     0.06285   \n",
       "...                                             ...   \n",
       "5648079                                    -0.00104   \n",
       "5648080                                    -0.05528   \n",
       "5648081                                    -0.10939   \n",
       "5648082                                    -0.13259   \n",
       "5648083                                    -0.07999   \n",
       "\n",
       "         all_day_ratio_single_tile_users                        baseline_name  \\\n",
       "4742224                          0.18910  full_february_except_presidents_day   \n",
       "4742225                          0.15003  full_february_except_presidents_day   \n",
       "4742226                          0.14391  full_february_except_presidents_day   \n",
       "4742227                          0.16058  full_february_except_presidents_day   \n",
       "4742228                          0.15298  full_february_except_presidents_day   \n",
       "...                                  ...                                  ...   \n",
       "5648079                          0.12959  full_february_except_presidents_day   \n",
       "5648080                          0.16893  full_february_except_presidents_day   \n",
       "5648081                          0.21618  full_february_except_presidents_day   \n",
       "5648082                          0.16558  full_february_except_presidents_day   \n",
       "5648083                          0.14719  full_february_except_presidents_day   \n",
       "\n",
       "        baseline_type  \n",
       "4742224   DAY_OF_WEEK  \n",
       "4742225   DAY_OF_WEEK  \n",
       "4742226   DAY_OF_WEEK  \n",
       "4742227   DAY_OF_WEEK  \n",
       "4742228   DAY_OF_WEEK  \n",
       "...               ...  \n",
       "5648079   DAY_OF_WEEK  \n",
       "5648080   DAY_OF_WEEK  \n",
       "5648081   DAY_OF_WEEK  \n",
       "5648082   DAY_OF_WEEK  \n",
       "5648083   DAY_OF_WEEK  \n",
       "\n",
       "[905860 rows x 9 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_mobility_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_contiguous = fb_mobility_us.index[fb_mobility_us['polygon_id'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_contiguous = fb_mobility_us.loc[idx_contiguous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229.68892669677734\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for index, row in fb_mobility_contiguous.iterrows():\n",
    "    relative_df.loc[row['polygon_id']][row['ds']] = row['all_day_bing_tiles_visited_relative_change']\n",
    "    ratio_df.loc[row['polygon_id']][row['ds']] = row['all_day_ratio_single_tile_users']\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2020-03-01</th>\n",
       "      <th>2020-03-02</th>\n",
       "      <th>2020-03-03</th>\n",
       "      <th>2020-03-04</th>\n",
       "      <th>2020-03-05</th>\n",
       "      <th>2020-03-06</th>\n",
       "      <th>2020-03-07</th>\n",
       "      <th>2020-03-08</th>\n",
       "      <th>2020-03-09</th>\n",
       "      <th>2020-03-10</th>\n",
       "      <th>...</th>\n",
       "      <th>2021-02-21</th>\n",
       "      <th>2021-02-22</th>\n",
       "      <th>2021-02-23</th>\n",
       "      <th>2021-02-24</th>\n",
       "      <th>2021-02-25</th>\n",
       "      <th>2021-02-26</th>\n",
       "      <th>2021-02-27</th>\n",
       "      <th>2021-02-28</th>\n",
       "      <th>2021-03-01</th>\n",
       "      <th>2021-03-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01001</th>\n",
       "      <td>0.04845</td>\n",
       "      <td>0.03275</td>\n",
       "      <td>0.05122</td>\n",
       "      <td>-0.04741</td>\n",
       "      <td>0.06285</td>\n",
       "      <td>0.03601</td>\n",
       "      <td>0.08057</td>\n",
       "      <td>0.05780</td>\n",
       "      <td>0.09284</td>\n",
       "      <td>-0.01149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>-0.01463</td>\n",
       "      <td>-0.03792</td>\n",
       "      <td>0.01120</td>\n",
       "      <td>-0.07577</td>\n",
       "      <td>0.01120</td>\n",
       "      <td>-0.06447</td>\n",
       "      <td>-0.03924</td>\n",
       "      <td>-0.09289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01003</th>\n",
       "      <td>0.14298</td>\n",
       "      <td>0.06573</td>\n",
       "      <td>0.09793</td>\n",
       "      <td>-0.04300</td>\n",
       "      <td>0.03726</td>\n",
       "      <td>0.02851</td>\n",
       "      <td>0.05436</td>\n",
       "      <td>0.12925</td>\n",
       "      <td>0.08460</td>\n",
       "      <td>0.09411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00171</td>\n",
       "      <td>-0.01074</td>\n",
       "      <td>-0.00187</td>\n",
       "      <td>-0.00520</td>\n",
       "      <td>0.03830</td>\n",
       "      <td>-0.01854</td>\n",
       "      <td>-0.04047</td>\n",
       "      <td>0.08357</td>\n",
       "      <td>0.00327</td>\n",
       "      <td>-0.09911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01005</th>\n",
       "      <td>0.19482</td>\n",
       "      <td>0.04019</td>\n",
       "      <td>-0.04098</td>\n",
       "      <td>-0.09939</td>\n",
       "      <td>-0.07579</td>\n",
       "      <td>0.01357</td>\n",
       "      <td>0.07628</td>\n",
       "      <td>0.15712</td>\n",
       "      <td>0.07074</td>\n",
       "      <td>-0.01814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.19003</td>\n",
       "      <td>-0.17879</td>\n",
       "      <td>-0.18552</td>\n",
       "      <td>-0.13591</td>\n",
       "      <td>-0.04956</td>\n",
       "      <td>-0.15831</td>\n",
       "      <td>-0.07127</td>\n",
       "      <td>-0.00472</td>\n",
       "      <td>-0.07938</td>\n",
       "      <td>-0.22962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01007</th>\n",
       "      <td>0.07842</td>\n",
       "      <td>0.05036</td>\n",
       "      <td>0.14394</td>\n",
       "      <td>-0.06774</td>\n",
       "      <td>0.00058</td>\n",
       "      <td>0.08248</td>\n",
       "      <td>0.09590</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.12007</td>\n",
       "      <td>0.05285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.06006</td>\n",
       "      <td>-0.04596</td>\n",
       "      <td>-0.03915</td>\n",
       "      <td>-0.08903</td>\n",
       "      <td>-0.04926</td>\n",
       "      <td>-0.06718</td>\n",
       "      <td>0.08553</td>\n",
       "      <td>-0.00576</td>\n",
       "      <td>0.00210</td>\n",
       "      <td>-0.01632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01009</th>\n",
       "      <td>0.03646</td>\n",
       "      <td>0.00816</td>\n",
       "      <td>0.08475</td>\n",
       "      <td>-0.02877</td>\n",
       "      <td>-0.02288</td>\n",
       "      <td>0.03908</td>\n",
       "      <td>0.05726</td>\n",
       "      <td>-0.03108</td>\n",
       "      <td>0.10135</td>\n",
       "      <td>0.00426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07313</td>\n",
       "      <td>-0.06065</td>\n",
       "      <td>-0.02379</td>\n",
       "      <td>-0.02874</td>\n",
       "      <td>-0.03835</td>\n",
       "      <td>-0.14458</td>\n",
       "      <td>-0.06133</td>\n",
       "      <td>-0.05191</td>\n",
       "      <td>-0.03508</td>\n",
       "      <td>-0.11645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2020-03-01  2020-03-02  2020-03-03  2020-03-04  2020-03-05  2020-03-06  \\\n",
       "01001     0.04845     0.03275     0.05122    -0.04741     0.06285     0.03601   \n",
       "01003     0.14298     0.06573     0.09793    -0.04300     0.03726     0.02851   \n",
       "01005     0.19482     0.04019    -0.04098    -0.09939    -0.07579     0.01357   \n",
       "01007     0.07842     0.05036     0.14394    -0.06774     0.00058     0.08248   \n",
       "01009     0.03646     0.00816     0.08475    -0.02877    -0.02288     0.03908   \n",
       "\n",
       "       2020-03-07  2020-03-08  2020-03-09  2020-03-10  ...  2021-02-21  \\\n",
       "01001     0.08057     0.05780     0.09284    -0.01149  ...     0.00018   \n",
       "01003     0.05436     0.12925     0.08460     0.09411  ...    -0.00171   \n",
       "01005     0.07628     0.15712     0.07074    -0.01814  ...    -0.19003   \n",
       "01007     0.09590     0.09463     0.12007     0.05285  ...    -0.06006   \n",
       "01009     0.05726    -0.03108     0.10135     0.00426  ...    -0.07313   \n",
       "\n",
       "       2021-02-22  2021-02-23  2021-02-24  2021-02-25  2021-02-26  2021-02-27  \\\n",
       "01001     0.00066    -0.01463    -0.03792     0.01120    -0.07577     0.01120   \n",
       "01003    -0.01074    -0.00187    -0.00520     0.03830    -0.01854    -0.04047   \n",
       "01005    -0.17879    -0.18552    -0.13591    -0.04956    -0.15831    -0.07127   \n",
       "01007    -0.04596    -0.03915    -0.08903    -0.04926    -0.06718     0.08553   \n",
       "01009    -0.06065    -0.02379    -0.02874    -0.03835    -0.14458    -0.06133   \n",
       "\n",
       "       2021-02-28  2021-03-01  2021-03-02  \n",
       "01001    -0.06447    -0.03924    -0.09289  \n",
       "01003     0.08357     0.00327    -0.09911  \n",
       "01005    -0.00472    -0.07938    -0.22962  \n",
       "01007    -0.00576     0.00210    -0.01632  \n",
       "01009    -0.05191    -0.03508    -0.11645  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3103, 367) (3103, 367)\n"
     ]
    }
   ],
   "source": [
    "print(relative_df.shape , ratio_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute FB mobility dataframes\n",
    "The two dataframes above have a lot of Nan values which should be imputed by state average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df = data[['GEOID', 'STATEFP']].merge(ratio_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_ratio_df.iloc[:,2:].columns:\n",
    "    temp_ratio_df[col] = temp_ratio_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_ratio_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df = data[['GEOID', 'STATEFP']].merge(relative_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_relative_df.iloc[:,2:].columns:\n",
    "    temp_relative_df[col] = temp_relative_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_relative_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth = temp_relative_df.copy()\n",
    "ratio_df_smooth = temp_ratio_df.copy()\n",
    "\n",
    "relative_df_smooth.iloc[:,2:] = relative_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()\n",
    "ratio_df_smooth.iloc[:,2:] = ratio_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2020-03-01', '2020-03-02', '2020-03-03', '2020-03-04', '2020-03-05',\n",
       "       '2020-03-06', '2020-03-07', '2020-03-08', '2020-03-09', '2020-03-10',\n",
       "       ...\n",
       "       '2021-02-21', '2021-02-22', '2021-02-23', '2021-02-24', '2021-02-25',\n",
       "       '2021-02-26', '2021-02-27', '2021-02-28', '2021-03-01', '2021-03-02'],\n",
       "      dtype='object', length=367)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_df_smooth.iloc[:,2:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3103, 369)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_df_smooth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Social Proximity to Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df = pd.read_csv(input_directory + 'SCI_matrix.csv', dtype={'Unnamed: 0':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized SCI. It is calculated by dividing all the columns of the sci_matrix by the sum of the rpw\n",
    "# This would give us the second term in social proximity formula above\n",
    "\n",
    "sci_matrix_normal = SCI_df.div(SCI_df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BehzadVahediTorghabe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# set diagonal to zero\n",
    "sci_matrix_normal.values[[np.arange(sci_matrix_normal.shape[0])]*2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The matrix above is created for the entire US, but we are using contiguous US data here, therefore some rows and columns should be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[]\n",
    "\n",
    "for index in sci_matrix_normal.index:\n",
    "    if not index in contiguous_fips:\n",
    "        to_drop.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3103, 3103)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_matrix_normal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add SafeGraph mobility features\n",
    "\n",
    "Updated based on forecast hub dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_mobility = pd.read_csv(input_directory + 'safegraph_mobility.csv', dtype={'county_fips':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_contiguous = safegraph_mobility[safegraph_mobility['county_fips'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3103"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(safegraph_contiguous['county_fips'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cols = safegraph_contiguous.drop(['start_date', 'end_date', 'base_start', 'base_end'], axis=1)\n",
    "safegraph_metrics = temp_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(safegraph_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp = pd.read_csv(input_directory + 'max_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp = pd.read_csv(input_directory + 'min_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return FCI-normal table for the input date\n",
    "# set path_to_fci to where FCI matrices are stored\n",
    "def get_normal_fci(date):\n",
    "    path_to_fci = './output/' + str(date.year) + '/'+ date.strftime('%m') + \n",
    "                '/FCI_normal/' + date.strftime('%Y-%m-%d') + '-FCI-normal.csv'\n",
    "    fci_norm = pd.read_csv(path_to_fci, dtype={'Unnamed: 0':str})\n",
    "    fci_norm.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    to_drop=[]\n",
    "\n",
    "    for index in fci_norm.index:\n",
    "        if not index in contiguous_fips:\n",
    "            to_drop.append(index)\n",
    "            \n",
    "    fci_norm.drop(to_drop, inplace=True)\n",
    "    fci_norm.drop(to_drop, axis=1, inplace=True)\n",
    "    return fci_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average FPC using the end date and the start date of the week\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_FPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        normal_fci = get_normal_fci(date)\n",
    "        normal_fci = normal_fci.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_fci['fpc_'+ date_str] = np.dot(normal_fci.iloc[:,:number_counties], normal_fci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_fci['mean_fpc'] = normal_fci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_fci[['GEOID','mean_fpc']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average SPC\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_SPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        \n",
    "        normal_sci = sci_matrix_normal.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_sci['spc_'+ date_str] = np.dot(normal_sci.iloc[:,:number_counties], normal_sci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_sci['mean_spc'] = normal_sci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_sci[['GEOID','mean_spc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input to this fuction should be of type datetime.\n",
    "# returns a subset of FB movement range dfs based on the given week\n",
    "def weekly_fb_mobility(end_date, start_date, df):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    dates_str=[]\n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        dates_str.append(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    return df[['GEOID', *dates_str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate slope features\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def linear_reg(week_df):\n",
    "    \n",
    "    x = np.arange(1,(week_df.shape[1]),1)\n",
    "    x = (x - np.mean(x))/ np.std(x)\n",
    "    \n",
    "    slopes=[]\n",
    "    \n",
    "    for index, row in week_df.iloc[:,1:].iterrows():\n",
    "        y = row\n",
    "        y = (y - np.mean(y))/ np.std(y)\n",
    "        slopes.append(linregress(x, y)[0])\n",
    "        \n",
    "    week_df.loc[:,'slope'] = slopes\n",
    "    return week_df[['GEOID','slope']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function to combine all features generated above\n",
    "\n",
    "This function generates a dataframe and for a given date, will add the following features to the dataframe\n",
    "\n",
    "- incidence rate data\n",
    "- FB mobility data (ratio, relative)\n",
    "- SPC (facebook SCI and incidence rates)\n",
    "- SafeGraph mobility \n",
    "- FPC (FCI and incidence rate)\n",
    "\n",
    "For each period, there is a 5 week difference between the actual date (t) and the start of the second lag. For example if `T: Oct 1 (Sep 24 to Oct 1)`, then `T-1: Sep 10 to Sep 24`, and `T-2: August 27 to Sep 10`.\n",
    "\n",
    "Since the earliest day for which we have FB mobility data is March 1, the rearliest  (end) date for T will be April 5th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_contiguous = data[['FIPS','STATEFP','COUNTYFP','GEOID']].merge(covid_df, on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_data(date):\n",
    "    global data\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=6)\n",
    "    \n",
    "    T_1_end = T_start - timedelta(days=1)\n",
    "    T_1_start = T_1_end - timedelta(days=6)\n",
    "    \n",
    "    T_2_end = T_1_start - timedelta(days=1)\n",
    "    T_2_start = T_2_end - timedelta(days=6)\n",
    "    \n",
    "    T_3_end = T_2_start - timedelta(days=1)\n",
    "    T_3_start = T_3_end - timedelta(days=6)\n",
    "    \n",
    "    T_4_end = T_3_start - timedelta(days=1)\n",
    "    T_4_start = T_4_end - timedelta(days=6)\n",
    "    \n",
    "    # These dates are used for cumulative cases (Saturday to Saturday)\n",
    "    T_start_case = T_end - timedelta(days=7)\n",
    "    T_1_start_case = T_1_end - timedelta(days=7)\n",
    "    T_2_start_case = T_2_end - timedelta(days=7)\n",
    "    T_3_start_case = T_3_end - timedelta(days=7)\n",
    "    T_4_start_case = T_4_end - timedelta(days=7)\n",
    "    \n",
    "\n",
    "    dates = [T_end.strftime('%Y-%m-%d'), T_start.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str = [T_end, T_start,\n",
    "             T_1_end, T_1_start, \n",
    "             T_2_end, T_2_start,\n",
    "             T_3_end, T_3_start, \n",
    "             T_4_end, T_4_start]\n",
    "    \n",
    "    dates_case = [T_end.strftime('%Y-%m-%d'), T_start_case.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start_case.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start_case.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start_case.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start_case.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str_case = [T_end, T_start_case,\n",
    "             T_1_end, T_1_start_case, \n",
    "             T_2_end, T_2_start_case,\n",
    "             T_3_end, T_3_start_case, \n",
    "             T_4_end, T_4_start_case]\n",
    "\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['date_end_period'] = T_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_period'] = T_start.strftime('%Y-%m-%d')\n",
    "    temp['date_end_lag'] = T_1_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_lag'] = T_4_start.strftime('%Y-%m-%d')\n",
    "    \n",
    "    time_periods = ['T_end', 'T_start', 'T_1_end', 'T_1_start','T_2_end','T_2_start',\n",
    "                    'T_3_end','T_3_start','T_4_end','T_4_start']\n",
    "    i = 0\n",
    "    for period in time_periods:\n",
    "        \n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['GEOID',dates_case[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        temp['inc_rate_' + period] = temp['case_'+ period] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        temp = temp.merge(relative_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'relative_'+ period}, inplace=True) \n",
    "\n",
    "\n",
    "        temp = temp.merge(ratio_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'ratio_'+ period}, inplace=True)\n",
    "        \n",
    "        # The same date is used as the input to weekly_mean_SPC function to calculate\n",
    "        # SPC for that given date (instead of an average over a period)\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_'+ period}, inplace=True)\n",
    "\n",
    "        # simiar to SPC, add FPC values\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged SPC (defined as log(delta incidence rate)*sci/sum(sci))\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_logged_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged FPC (defined as log(delta incidence rate)*fci/sum(fci))\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_logged_'+ period}, inplace=True)\n",
    "\n",
    "        # add raw John Hopkins case data\n",
    "        temp = temp.merge(jh_covid_df[['FIPS',dates_non_str_case[i].strftime('%#m/%#d/%y')]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_non_str_case[i].strftime('%#m/%#d/%y'):'case_JH_'+ period}, inplace=True)\n",
    "        \n",
    "        # add smoothed John Hopkins case data\n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS',dates_case[i]]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_JH_smoothed_'+ period}, inplace=True)\n",
    "       \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "    \n",
    "    j = 0\n",
    "    for period in times:\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + time_periods[j]] - temp['inc_rate_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_REL_MOB_' + period] = temp['relative_' + time_periods[j]] - temp['relative_' + time_periods[j+1]]\n",
    "        temp['DELTA_RATIO_MOB_' + period] = temp['ratio_' + time_periods[j]] - temp['ratio_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_SPC_' + period] = temp['SPC_' + time_periods[j]] - temp['SPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_' + period] = temp['FPC_' + time_periods[j]] - temp['FPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_SPC_LOGGED_' + period] = temp['SPC_logged_' + time_periods[j]] - temp['SPC_logged_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_LOGGED_' + period] = temp['FPC_logged_' + time_periods[j]] - temp['FPC_logged_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_CASE_JH_' + period] = temp['case_JH_'+ time_periods[j]] - temp['case_JH_'+ time_periods[j+1]]\n",
    "        temp['DELTA_CASE_JH_SMOOTH_' + period] = temp['case_JH_smoothed_'+ time_periods[j]] - \n",
    "                                                 temp['case_JH_smoothed_'+ time_periods[j+1]]\n",
    "        \n",
    "        # mean incidence rate is calculated between Sunday and Saturday\n",
    "        temp['MEAN_INC_RATE_' + period] = covid_df_contiguous[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1) / temp['POPULATION'] * 10000\n",
    "        temp['MEAN_REL_MOB_' + period] = relative_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "        temp['MEAN_RATIO_MOB_' + period] = ratio_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "\n",
    "        \n",
    "        # add Safegraph mobility features\n",
    "        safegraph_data = safegraph_contiguous[safegraph_contiguous['end_date']==dates[j]][safegraph_metrics]\n",
    "        temp = temp.merge(safegraph_data, left_on='GEOID', right_on='county_fips')\n",
    "        \n",
    "        rename_dict = dict()\n",
    "        for col in safegraph_metrics[1:]:\n",
    "            rename_dict[col] = col + '_' + period\n",
    "            \n",
    "        temp.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add MEAN_FPC\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_FPC \n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add FB mobility slopes\n",
    "        ratio_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], ratio_df_smooth))\n",
    "        temp = temp.merge(ratio_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_RATIO_MOB_'+ period}, inplace=True)\n",
    "        \n",
    "        relative_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], relative_df_smooth))\n",
    "        temp = temp.merge(relative_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_REL_MOB_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # add temperature features\n",
    "        # to update for the new dates, min and max temperature are used with one day offset\n",
    "        adj_temp_date = (dates_non_str[j] + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(max_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MAX_TEMP_'+ period}, inplace=True)\n",
    "        \n",
    "        temp = temp.merge(min_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MIN_TEMP_'+ period}, inplace=True)\n",
    "\n",
    "        j += 2\n",
    "\n",
    "    output_df = temp.copy()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "week_counter = 0\n",
    "df_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_list.append(add_lagged_data(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    \n",
    "    end_date -= timedelta(weeks=1)\n",
    "    week_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of weeks for which we have features\n",
    "final_df.shape[0]/3103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_rows', 200)\n",
    "final_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 400)\n",
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_columns = data_to_save.columns[data_to_save.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values by state average\n",
    "for col in data_to_save[na_columns].columns:\n",
    "    data_to_save[col] = data_to_save.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "\n",
    "for period in times:\n",
    "    data_to_save['LOG_DELTA_INC_RATE_' + period] = np.log(data_to_save['DELTA_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_MEAN_INC_RATE_' + period] = np.log(data_to_save['MEAN_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_SPC_' + period] = np.log(data_to_save['DELTA_SPC_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_FPC_' + period] = np.log(data_to_save['DELTA_FPC_' + period] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = [\n",
    "'GEOID',\n",
    "'NAME',\n",
    "'State_Name',\n",
    "'STATEFP', \n",
    "'COUNTYFP', \n",
    "'date_end_period',\n",
    "'date_start_period',\n",
    "'date_end_lag',\n",
    "'date_start_lag',\n",
    "'LOG_DELTA_INC_RATE_T',\n",
    "'PCT_MALE',\n",
    "'PCT_BLACK',\n",
    "'PCT_HISPAN', \n",
    "'PCT_AMIND',\n",
    "'PCT_RURAL',\n",
    "'PCT_COL_DE' ,\n",
    "'PCT_TRUMP_',\n",
    "'MED_HOS_IN',\n",
    "'POPULATION',\n",
    "'DELTA_CASE_JH_T',\n",
    "'DELTA_CASE_JH_SMOOTH_T'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_cols=[\n",
    "'LOG_DELTA_INC_RATE_T_',\n",
    "'DELTA_REL_MOB_T_',\n",
    "'DELTA_RATIO_MOB_T_',\n",
    "'DELTA_SPC_T_',\n",
    "'DELTA_SPC_LOGGED_T_',\n",
    "'DELTA_FPC_T_',\n",
    "'DELTA_FPC_LOGGED_T_',\n",
    "'LOG_MEAN_INC_RATE_T_',\n",
    "'MEAN_REL_MOB_T_',\n",
    "'MEAN_RATIO_MOB_T_',\n",
    "'MEAN_FPC_T_',\n",
    "'MEAN_SPC_T_',\n",
    "'SLOPE_RATIO_MOB_T_',\n",
    "'SLOPE_REL_MOB_T_',\n",
    "'MAX_TEMP_T_',\n",
    "'MIN_TEMP_T_',\n",
    "'pct_completely_home_device_count_current_T_',\n",
    "'pct_full_time_work_behavior_devices_current_T_',\n",
    "'pct_part_time_work_behavior_devices_current_T_',\n",
    "'pct_delivery_behavior_devices_current_T_',\n",
    "'distance_traveled_from_home_current_T_',\n",
    "'median_home_dwell_time_current_T_',\n",
    "'pct_completely_home_device_count_baselined_T_',\n",
    "'pct_full_time_work_behavior_devices_baselined_T_',\n",
    "'pct_part_time_work_behavior_devices_baselined_T_',\n",
    "'pct_delivery_behavior_devices_baselined_T_',\n",
    "'distance_traveled_from_home_baselined_T_',\n",
    "'median_home_dwell_time_baselined_T_',\n",
    "'pct_completely_home_device_count_slope_T_',\n",
    "'pct_full_time_work_behavior_devices_slope_T_',\n",
    "'pct_part_time_work_behavior_devices_slope_T_',\n",
    "'pct_delivery_behavior_devices_slope_T_',\n",
    "'distance_traveled_from_home_slope_T_',\n",
    "'median_home_dwell_time_slope_T_',\n",
    "'DELTA_CASE_JH_T_',\n",
    "'MEAN_SPC_LOGGED_T_',\n",
    "'MEAN_FPC_LOGGED_T_'\n",
    "]\n",
    "\n",
    "for i in range(1,5):\n",
    "    for col in additional_cols:\n",
    "        final_cols.append(col+str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = data_to_save[final_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('./output/all_features_updated_incidence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes for 2, 3, and 4-week predictions\n",
    "\n",
    "in this dataframe, the target variables is the the number of cumulative cases in 2, 3, and 4 weeks ahead, denoted by `LOG_DELTA_INC_RATE_T_14`, `LOG_DELTA_INC_RATE_T_21`, and `LOG_DELTA_INC_RATE_T_28` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_y(date):\n",
    "    global output, jh_covid_df\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=7)\n",
    "    \n",
    "    T_start_period = (T_end - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    T_14 =  T_end + timedelta(days=7)\n",
    "    T_21 =  T_end + timedelta(days=14)\n",
    "    T_28 =  T_end + timedelta(days=21)\n",
    "    \n",
    "\n",
    "    dates_non_str = [T_end, T_start, T_14, T_21, T_28]\n",
    "    \n",
    "    dates = [item.strftime('%Y-%m-%d') for item in dates_non_str]\n",
    "    \n",
    "    dates_jh = [item.strftime('%#m/%#d/%y') for item in dates_non_str]\n",
    "    \n",
    "    \n",
    "    periods = ['T_end', 'T_start', 'T_14', 'T_21', 'T_28']\n",
    "    \n",
    "    temp = output.loc[(output.date_end_period==dates[0]) & (output.date_start_period==T_start_period)].copy()\n",
    "    \n",
    "    temp['FIPS'] = temp['GEOID'].astype(int)\n",
    "    \n",
    "    #print('check 1 {}'.format(temp.shape))\n",
    "    temp['target_date_2wk'] = T_14.strftime('%Y-%m-%d')\n",
    "    temp['target_date_3wk'] = T_21.strftime('%Y-%m-%d')\n",
    "    temp['target_date_4wk'] = T_28.strftime('%Y-%m-%d')\n",
    "        \n",
    "    temp = temp.merge(covid_df_contiguous[['GEOID',*dates]], on='GEOID', how='left')\n",
    "    \n",
    "    temp = temp.merge(jh_covid_df[['FIPS',*dates_jh]], on='FIPS', how='left')\n",
    "    #print('check 2 {}'.format(temp.shape))\n",
    "\n",
    "    for period, date in zip(periods, dates):\n",
    "        temp['inc_rate_' + period] = temp[date] / temp['POPULATION'] * 10000\n",
    "\n",
    "\n",
    "    for period, date in zip(periods[-3:], dates[-3:]):\n",
    "        temp['DELTA_CASE_SMOOTHED_' + period] = temp[date] - temp[dates[1]]\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + period] - temp['inc_rate_T_start']\n",
    "        temp['LOG_DELTA_INC_RATE_' + period] = np.log(temp['DELTA_INC_RATE_' + period] + 1)\n",
    "    \n",
    "    for period, date in zip(periods[-3:], dates_jh[-3:]):\n",
    "        temp['DELTA_CASE_JH_' + period] = temp[date] - temp[dates_jh[1]]\n",
    "    \n",
    "    temp['DELTA_CASE_JH_T'] = temp[dates_jh[0]] - temp[dates_jh[1]]\n",
    "        \n",
    "    \n",
    "    \n",
    "    cols = ['target_date_2wk','LOG_DELTA_INC_RATE_T_14', \n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28' ]\n",
    "    #print('check 3 {}'.format(temp.shape))\n",
    "    \n",
    "    return temp[[*output.columns,'DELTA_CASE_JH_T',\n",
    "            'target_date_2wk','LOG_DELTA_INC_RATE_T_14', 'DELTA_CASE_SMOOTHED_T_14', 'DELTA_CASE_JH_T_14',\n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21', 'DELTA_CASE_SMOOTHED_T_21', 'DELTA_CASE_JH_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28', 'DELTA_CASE_SMOOTHED_T_28', 'DELTA_CASE_JH_T_28']]\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "df_lagged_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_lagged_list.append(add_lagged_y(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    end_date -= timedelta(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged = pd.concat(df_lagged_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.shape, df_lagged.shape[0]/3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.to_csv('./output/all_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
