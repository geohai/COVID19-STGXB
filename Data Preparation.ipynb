{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This notebook is used to clean up, preprocess, and aggregate data necessary for STXGB models.\n",
    "\n",
    "Please refer to the `Method` section of the article and Supplementary Information document for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set date string formatting based on operating system\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    conversion_format = '%#m/%#d/%y'\n",
    "else:\n",
    "    conversion_format = '%-m/%-d/%y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = './input/'\n",
    "data = gpd.read_file( input_directory + 'Contiguous_US.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global number_counties \n",
    "number_counties = data.shape[0] #3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(by='GEOID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COVID data and apply smoothing \n",
    "\n",
    "To alleviate inconsistencies in reporting COVID-19 cases, we apply a 7-day moving average to the case data published by JHU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_JH_covid_data(target, smooth):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    --------------\n",
    "        target: str\n",
    "            the target variable: either 'case' or 'death'\n",
    "            \n",
    "        smooth: bool\n",
    "            wether to smooth the data frame or not.\n",
    "            The smoothing is done by using a 7-day rolling average   \n",
    "    '''\n",
    "    \n",
    "    assert isinstance(smooth, bool), \"Smooth must be a boolean variable!\"\n",
    "    \n",
    "    base_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/'\n",
    "    \n",
    "    \n",
    "    if target == 'case':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_']\n",
    "\n",
    "    elif target=='death':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_','Population']\n",
    "    else:\n",
    "        print(\"invalid argument for target. Acceptable values are: 'case' or 'death'\")\n",
    "        return None\n",
    "\n",
    "    jh_covid_df = pd.read_csv(jh_data_url)\n",
    "\n",
    "    # preprocessing JH COVID data\n",
    "    jh_covid_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    jh_covid_df['FIPS'] = jh_covid_df['FIPS'].astype('int64')\n",
    "\n",
    "    jh_covid_df.drop(columns=cols_to_drp, inplace=True)\n",
    "\n",
    "    #Important: check to see the column index is adherent to the imported df\n",
    "\n",
    "    first_date = datetime.strptime(jh_covid_df.columns[4], '%m/%d/%y').date()\n",
    "\n",
    "    last_date = datetime.strptime(jh_covid_df.columns[-1], '%m/%d/%y').date()\n",
    "\n",
    "\n",
    "    current_date = last_date\n",
    "\n",
    "    previous_date = last_date - timedelta (days=1)\n",
    "\n",
    "\n",
    "    while current_date > first_date:\n",
    "\n",
    "        #For unix, replace # with - in the time format\n",
    "\n",
    "        current_col = current_date.strftime(conversion_format) #replace # with - in Mac or Linux\n",
    "\n",
    "        previous_col = previous_date.strftime(conversion_format)\n",
    "\n",
    "        jh_covid_df[previous_col] = np.where(jh_covid_df[previous_col] > jh_covid_df[current_col], \n",
    "                                             jh_covid_df[current_col], jh_covid_df[previous_col])\n",
    "\n",
    "        current_date = current_date - timedelta(days=1)\n",
    "\n",
    "        previous_date = previous_date - timedelta(days=1)\n",
    "        \n",
    "    \n",
    "    if smooth:\n",
    "        jh_covid_df.iloc[:,4:] = jh_covid_df.iloc[:,4:].rolling(7,min_periods=1,axis=1).mean()\n",
    "\n",
    "\n",
    "    return jh_covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = get_JH_covid_data('case', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Facebook Movement Range Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility = pd.read_csv(input_directory + 'movement-range-2021-03-02.txt', sep=\"\\t\", dtype={'polygon_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us = fb_mobility[fb_mobility['country']=='USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique counties for which we have at least one day of data\n",
    "len(fb_mobility_us['polygon_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting Counties in the contiguous US for which there is no data in FB mobility\n",
    "contiguous_fips = set(data['GEOID']) # number of unique fips: 3103\n",
    "mobility_fips = set(fb_mobility_us['polygon_id']) # number of unique fips: 2694\n",
    "i = 0\n",
    "missing_fips = []\n",
    "for fips in contiguous_fips:\n",
    "    if (fips in mobility_fips):\n",
    "        i+=1\n",
    "    else:\n",
    "        missing_fips.append(fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Counties in the contiguous US for which there is no data in FB mobility\n",
    "len(missing_fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe as transpose of the above, with days as columns and counties as rows\n",
    "\n",
    "relative_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)\n",
    "ratio_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_contiguous = fb_mobility_us.index[fb_mobility_us['polygon_id'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_contiguous = fb_mobility_us.loc[idx_contiguous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for index, row in fb_mobility_contiguous.iterrows():\n",
    "    relative_df.loc[row['polygon_id']][row['ds']] = row['all_day_bing_tiles_visited_relative_change']\n",
    "    ratio_df.loc[row['polygon_id']][row['ds']] = row['all_day_ratio_single_tile_users']\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relative_df.shape , ratio_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute FB mobility dataframes\n",
    "The two dataframes above have a lot of Nan values which should be imputed by state average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df = data[['GEOID', 'STATEFP']].merge(ratio_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_ratio_df.iloc[:,2:].columns:\n",
    "    temp_ratio_df[col] = temp_ratio_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df = data[['GEOID', 'STATEFP']].merge(relative_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_relative_df.iloc[:,2:].columns:\n",
    "    temp_relative_df[col] = temp_relative_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth = temp_relative_df.copy()\n",
    "ratio_df_smooth = temp_ratio_df.copy()\n",
    "\n",
    "relative_df_smooth.iloc[:,2:] = relative_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()\n",
    "ratio_df_smooth.iloc[:,2:] = ratio_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.iloc[:,2:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Social Proximity to Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df = pd.read_csv(input_directory + 'SCI_matrix.csv', dtype={'Unnamed: 0':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized SCI. It is calculated by dividing all the columns of the sci_matrix by the sum of the rpw\n",
    "# This would give us the second term in social proximity formula above\n",
    "\n",
    "sci_matrix_normal = SCI_df.div(SCI_df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set diagonal to zero\n",
    "sci_matrix_normal.values[[np.arange(sci_matrix_normal.shape[0])]*2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The matrix above is created for the entire US, but we are using contiguous US data here, therefore some rows and columns should be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[]\n",
    "\n",
    "for index in sci_matrix_normal.index:\n",
    "    if not index in contiguous_fips:\n",
    "        to_drop.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['POP_DENSITY'] = data['POPULATION'] / data['ALAND']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add SafeGraph mobility features\n",
    "\n",
    "Updated based on forecast hub dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_mobility_21 = pd.read_csv(input_directory + 'weekly_df_extended_2021.csv', dtype={'county_fips':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_mobility = pd.read_csv(input_directory + 'weekly_df_updated_dates_feb20.csv', dtype={'county_fips':str})\n",
    "safegraph_mobility = pd.concat([safegraph_mobility, safegraph_mobility_21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_contiguous = safegraph_mobility[safegraph_mobility['county_fips'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_contiguous['county_fips'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cols = safegraph_contiguous.drop(['start_date', 'end_date', 'base_start', 'base_end'], axis=1)\n",
    "safegraph_metrics = temp_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp = pd.read_csv(input_directory + 'max_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp = pd.read_csv(input_directory + 'min_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return FCI-normal table for the input date\n",
    "# set path_to_fci to where FCI matrices are stored\n",
    "def get_normal_fci(date):\n",
    "    path_to_fci = './output/' + str(date.year) + '/'+ date.strftime('%m') + \n",
    "                '/FCI_normal/' + date.strftime('%Y-%m-%d') + '-FCI-normal.csv'\n",
    "    fci_norm = pd.read_csv(path_to_fci, dtype={'Unnamed: 0':str})\n",
    "    fci_norm.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    to_drop=[]\n",
    "\n",
    "    for index in fci_norm.index:\n",
    "        if not index in contiguous_fips:\n",
    "            to_drop.append(index)\n",
    "            \n",
    "    fci_norm.drop(to_drop, inplace=True)\n",
    "    fci_norm.drop(to_drop, axis=1, inplace=True)\n",
    "    return fci_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average FPC using the end date and the start date of the week\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_FPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        normal_fci = get_normal_fci(date)\n",
    "        normal_fci = normal_fci.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_fci['fpc_'+ date_str] = np.dot(normal_fci.iloc[:,:number_counties], normal_fci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_fci['mean_fpc'] = normal_fci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_fci[['GEOID','mean_fpc']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average SPC\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_SPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        \n",
    "        normal_sci = sci_matrix_normal.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_sci['spc_'+ date_str] = np.dot(normal_sci.iloc[:,:number_counties], normal_sci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_sci['mean_spc'] = normal_sci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_sci[['GEOID','mean_spc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input to this fuction should be of type datetime.\n",
    "# returns a subset of FB movement range dfs based on the given week\n",
    "def weekly_fb_mobility(end_date, start_date, df):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    dates_str=[]\n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        dates_str.append(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    return df[['GEOID', *dates_str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate slope features\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def linear_reg(week_df):\n",
    "    \n",
    "    x = np.arange(1,(week_df.shape[1]),1)\n",
    "    x = (x - np.mean(x))/ np.std(x)\n",
    "    \n",
    "    slopes=[]\n",
    "    \n",
    "    for index, row in week_df.iloc[:,1:].iterrows():\n",
    "        y = row\n",
    "        y = (y - np.mean(y))/ np.std(y)\n",
    "        slopes.append(linregress(x, y)[0])\n",
    "        \n",
    "    week_df.loc[:,'slope'] = slopes\n",
    "    return week_df[['GEOID','slope']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function to combine all features generated above\n",
    "\n",
    "This function generates a dataframe and for a given date, will add the following features to the dataframe\n",
    "\n",
    "- incidence rate data\n",
    "- FB mobility data (ratio, relative)\n",
    "- SPC (facebook SCI and incidence rates)\n",
    "- SafeGraph mobility \n",
    "- FPC (FCI and incidence rate)\n",
    "\n",
    "For each period, there is a 5 week difference between the actual date (t) and the start of the second lag. For example if `T: Oct 1 (Sep 24 to Oct 1)`, then `T-1: Sep 10 to Sep 24`, and `T-2: August 27 to Sep 10`.\n",
    "\n",
    "Since the earliest day for which we have FB mobility data is March 1, the rearliest  (end) date for T will be April 5th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_contiguous = data[['FIPS','STATEFP','COUNTYFP','GEOID']].merge(covid_df, on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_data(date):\n",
    "    global data\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=6)\n",
    "    \n",
    "    T_1_end = T_start - timedelta(days=1)\n",
    "    T_1_start = T_1_end - timedelta(days=6)\n",
    "    \n",
    "    T_2_end = T_1_start - timedelta(days=1)\n",
    "    T_2_start = T_2_end - timedelta(days=6)\n",
    "    \n",
    "    T_3_end = T_2_start - timedelta(days=1)\n",
    "    T_3_start = T_3_end - timedelta(days=6)\n",
    "    \n",
    "    T_4_end = T_3_start - timedelta(days=1)\n",
    "    T_4_start = T_4_end - timedelta(days=6)\n",
    "    \n",
    "    # These dates are used for cumulative cases (Saturday to Saturday)\n",
    "    T_start_case = T_end - timedelta(days=7)\n",
    "    T_1_start_case = T_1_end - timedelta(days=7)\n",
    "    T_2_start_case = T_2_end - timedelta(days=7)\n",
    "    T_3_start_case = T_3_end - timedelta(days=7)\n",
    "    T_4_start_case = T_4_end - timedelta(days=7)\n",
    "    \n",
    "\n",
    "    dates = [T_end.strftime('%Y-%m-%d'), T_start.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str = [T_end, T_start,\n",
    "             T_1_end, T_1_start, \n",
    "             T_2_end, T_2_start,\n",
    "             T_3_end, T_3_start, \n",
    "             T_4_end, T_4_start]\n",
    "    \n",
    "    dates_case = [T_end.strftime('%Y-%m-%d'), T_start_case.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start_case.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start_case.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start_case.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start_case.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str_case = [T_end, T_start_case,\n",
    "             T_1_end, T_1_start_case, \n",
    "             T_2_end, T_2_start_case,\n",
    "             T_3_end, T_3_start_case, \n",
    "             T_4_end, T_4_start_case]\n",
    "\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['date_end_period'] = T_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_period'] = T_start.strftime('%Y-%m-%d')\n",
    "    temp['date_end_lag'] = T_1_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_lag'] = T_4_start.strftime('%Y-%m-%d')\n",
    "    \n",
    "    time_periods = ['T_end', 'T_start', 'T_1_end', 'T_1_start','T_2_end','T_2_start',\n",
    "                    'T_3_end','T_3_start','T_4_end','T_4_start']\n",
    "    i = 0\n",
    "    for period in time_periods:\n",
    "        \n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['GEOID',dates_case[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        temp['inc_rate_' + period] = temp['case_'+ period] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        temp = temp.merge(relative_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'relative_'+ period}, inplace=True) \n",
    "\n",
    "\n",
    "        temp = temp.merge(ratio_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'ratio_'+ period}, inplace=True)\n",
    "        \n",
    "        # The same date is used as the input to weekly_mean_SPC function to calculate\n",
    "        # SPC for that given date (instead of an average over a period)\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_'+ period}, inplace=True)\n",
    "\n",
    "        # simiar to SPC, add FPC values\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged SPC (defined as log(delta incidence rate)*sci/sum(sci))\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_logged_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged FPC (defined as log(delta incidence rate)*fci/sum(fci))\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_logged_'+ period}, inplace=True)\n",
    "\n",
    "        # add raw John Hopkins case data\n",
    "        temp = temp.merge(jh_covid_df[['FIPS',dates_non_str_case[i].strftime('%#m/%#d/%y')]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_non_str_case[i].strftime('%#m/%#d/%y'):'case_JH_'+ period}, inplace=True)\n",
    "        \n",
    "        # add smoothed John Hopkins case data\n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS',dates_case[i]]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_JH_smoothed_'+ period}, inplace=True)\n",
    "       \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "    \n",
    "    j = 0\n",
    "    for period in times:\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + time_periods[j]] - temp['inc_rate_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_REL_MOB_' + period] = temp['relative_' + time_periods[j]] - temp['relative_' + time_periods[j+1]]\n",
    "        temp['DELTA_RATIO_MOB_' + period] = temp['ratio_' + time_periods[j]] - temp['ratio_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_SPC_' + period] = temp['SPC_' + time_periods[j]] - temp['SPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_' + period] = temp['FPC_' + time_periods[j]] - temp['FPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_SPC_LOGGED_' + period] = temp['SPC_logged_' + time_periods[j]] - temp['SPC_logged_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_LOGGED_' + period] = temp['FPC_logged_' + time_periods[j]] - temp['FPC_logged_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_CASE_JH_' + period] = temp['case_JH_'+ time_periods[j]] - temp['case_JH_'+ time_periods[j+1]]\n",
    "        temp['DELTA_CASE_JH_SMOOTH_' + period] = temp['case_JH_smoothed_'+ time_periods[j]] - \n",
    "                                                 temp['case_JH_smoothed_'+ time_periods[j+1]]\n",
    "        \n",
    "        # mean incidence rate is calculated between Sunday and Saturday\n",
    "        temp['MEAN_INC_RATE_' + period] = covid_df_contiguous[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1) / temp['POPULATION'] * 10000\n",
    "        temp['MEAN_REL_MOB_' + period] = relative_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "        temp['MEAN_RATIO_MOB_' + period] = ratio_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "\n",
    "        \n",
    "        # add Safegraph mobility features\n",
    "        safegraph_data = safegraph_contiguous[safegraph_contiguous['end_date']==dates[j]][safegraph_metrics]\n",
    "        temp = temp.merge(safegraph_data, left_on='GEOID', right_on='county_fips')\n",
    "        \n",
    "        rename_dict = dict()\n",
    "        for col in safegraph_metrics[1:]:\n",
    "            rename_dict[col] = col + '_' + period\n",
    "            \n",
    "        temp.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add MEAN_FPC\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_FPC \n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add FB mobility slopes\n",
    "        ratio_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], ratio_df_smooth))\n",
    "        temp = temp.merge(ratio_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_RATIO_MOB_'+ period}, inplace=True)\n",
    "        \n",
    "        relative_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], relative_df_smooth))\n",
    "        temp = temp.merge(relative_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_REL_MOB_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # add temperature features\n",
    "        # to update for the new dates, min and max temperature are used with one day offset\n",
    "        adj_temp_date = (dates_non_str[j] + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(max_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MAX_TEMP_'+ period}, inplace=True)\n",
    "        \n",
    "        temp = temp.merge(min_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MIN_TEMP_'+ period}, inplace=True)\n",
    "\n",
    "        j += 2\n",
    "\n",
    "    output_df = temp.copy()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "week_counter = 0\n",
    "df_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_list.append(add_lagged_data(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    \n",
    "    end_date -= timedelta(weeks=1)\n",
    "    week_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of weeks for which we have features\n",
    "final_df.shape[0]/3103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_rows', 200)\n",
    "final_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 400)\n",
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_columns = data_to_save.columns[data_to_save.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values by state average\n",
    "for col in data_to_save[na_columns].columns:\n",
    "    data_to_save[col] = data_to_save.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "\n",
    "for period in times:\n",
    "    data_to_save['LOG_DELTA_INC_RATE_' + period] = np.log(data_to_save['DELTA_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_MEAN_INC_RATE_' + period] = np.log(data_to_save['MEAN_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_SPC_' + period] = np.log(data_to_save['DELTA_SPC_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_FPC_' + period] = np.log(data_to_save['DELTA_FPC_' + period] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = [\n",
    "'GEOID',\n",
    "'NAME',\n",
    "'State_Name',\n",
    "'STATEFP', \n",
    "'COUNTYFP', \n",
    "'date_end_period',\n",
    "'date_start_period',\n",
    "'date_end_lag',\n",
    "'date_start_lag',\n",
    "'LOG_DELTA_INC_RATE_T',\n",
    "'POP_DENSITY',\n",
    "'PCT_MALE',\n",
    "'PCT_BLACK',\n",
    "'PCT_HISPAN', \n",
    "'PCT_AMIND',\n",
    "'PCT_RURAL',\n",
    "'PCT_COL_DE' ,\n",
    "'PCT_TRUMP_',\n",
    "'MED_HOS_IN',\n",
    "'POPULATION',\n",
    "'DELTA_CASE_JH_T',\n",
    "'DELTA_CASE_JH_SMOOTH_T'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_cols=[\n",
    "'LOG_DELTA_INC_RATE_T_',\n",
    "'DELTA_REL_MOB_T_',\n",
    "'DELTA_RATIO_MOB_T_',\n",
    "'DELTA_SPC_T_',\n",
    "'DELTA_SPC_LOGGED_T_',\n",
    "'DELTA_FPC_T_',\n",
    "'DELTA_FPC_LOGGED_T_',\n",
    "'LOG_MEAN_INC_RATE_T_',\n",
    "'MEAN_REL_MOB_T_',\n",
    "'MEAN_RATIO_MOB_T_',\n",
    "'MEAN_FPC_T_',\n",
    "'MEAN_SPC_T_',\n",
    "'SLOPE_RATIO_MOB_T_',\n",
    "'SLOPE_REL_MOB_T_',\n",
    "'MAX_TEMP_T_',\n",
    "'MIN_TEMP_T_',\n",
    "'pct_completely_home_device_count_current_T_',\n",
    "'pct_full_time_work_behavior_devices_current_T_',\n",
    "'pct_part_time_work_behavior_devices_current_T_',\n",
    "'pct_delivery_behavior_devices_current_T_',\n",
    "'distance_traveled_from_home_current_T_',\n",
    "'median_home_dwell_time_current_T_',\n",
    "'pct_completely_home_device_count_baselined_T_',\n",
    "'pct_full_time_work_behavior_devices_baselined_T_',\n",
    "'pct_part_time_work_behavior_devices_baselined_T_',\n",
    "'pct_delivery_behavior_devices_baselined_T_',\n",
    "'distance_traveled_from_home_baselined_T_',\n",
    "'median_home_dwell_time_baselined_T_',\n",
    "'pct_completely_home_device_count_slope_T_',\n",
    "'pct_full_time_work_behavior_devices_slope_T_',\n",
    "'pct_part_time_work_behavior_devices_slope_T_',\n",
    "'pct_delivery_behavior_devices_slope_T_',\n",
    "'distance_traveled_from_home_slope_T_',\n",
    "'median_home_dwell_time_slope_T_',\n",
    "'DELTA_CASE_JH_T_',\n",
    "'MEAN_SPC_LOGGED_T_',\n",
    "'MEAN_FPC_LOGGED_T_'\n",
    "]\n",
    "\n",
    "for i in range(1,5):\n",
    "    for col in additional_cols:\n",
    "        final_cols.append(col+str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = data_to_save[final_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('./output/all_features_updated_incidence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['date_end_period'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['date_start_period'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes for 2, 3, and 4-week predictions\n",
    "\n",
    "in this dataframe, the target variables is the the number of cumulative cases in 2, 3, and 4 weeks ahead, denoted by `LOG_DELTA_INC_RATE_T_14`, `LOG_DELTA_INC_RATE_T_21`, and `LOG_DELTA_INC_RATE_T_28` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_y(date):\n",
    "    global output, jh_covid_df\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=7)\n",
    "    \n",
    "    T_start_period = (T_end - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    T_14 =  T_end + timedelta(days=7)\n",
    "    T_21 =  T_end + timedelta(days=14)\n",
    "    T_28 =  T_end + timedelta(days=21)\n",
    "    \n",
    "\n",
    "    dates_non_str = [T_end, T_start, T_14, T_21, T_28]\n",
    "    \n",
    "    dates = [item.strftime('%Y-%m-%d') for item in dates_non_str]\n",
    "    \n",
    "    dates_jh = [item.strftime('%#m/%#d/%y') for item in dates_non_str]\n",
    "    \n",
    "    \n",
    "    periods = ['T_end', 'T_start', 'T_14', 'T_21', 'T_28']\n",
    "    \n",
    "    temp = output.loc[(output.date_end_period==dates[0]) & (output.date_start_period==T_start_period)].copy()\n",
    "    \n",
    "    temp['FIPS'] = temp['GEOID'].astype(int)\n",
    "    \n",
    "    #print('check 1 {}'.format(temp.shape))\n",
    "    temp['target_date_2wk'] = T_14.strftime('%Y-%m-%d')\n",
    "    temp['target_date_3wk'] = T_21.strftime('%Y-%m-%d')\n",
    "    temp['target_date_4wk'] = T_28.strftime('%Y-%m-%d')\n",
    "        \n",
    "    temp = temp.merge(covid_df_contiguous[['GEOID',*dates]], on='GEOID', how='left')\n",
    "    \n",
    "    temp = temp.merge(jh_covid_df[['FIPS',*dates_jh]], on='FIPS', how='left')\n",
    "    #print('check 2 {}'.format(temp.shape))\n",
    "\n",
    "    for period, date in zip(periods, dates):\n",
    "        temp['inc_rate_' + period] = temp[date] / temp['POPULATION'] * 10000\n",
    "\n",
    "\n",
    "    for period, date in zip(periods[-3:], dates[-3:]):\n",
    "        temp['DELTA_CASE_SMOOTHED_' + period] = temp[date] - temp[dates[1]]\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + period] - temp['inc_rate_T_start']\n",
    "        temp['LOG_DELTA_INC_RATE_' + period] = np.log(temp['DELTA_INC_RATE_' + period] + 1)\n",
    "    \n",
    "    for period, date in zip(periods[-3:], dates_jh[-3:]):\n",
    "        temp['DELTA_CASE_JH_' + period] = temp[date] - temp[dates_jh[1]]\n",
    "    \n",
    "    temp['DELTA_CASE_JH_T'] = temp[dates_jh[0]] - temp[dates_jh[1]]\n",
    "        \n",
    "    \n",
    "    \n",
    "    cols = ['target_date_2wk','LOG_DELTA_INC_RATE_T_14', \n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28' ]\n",
    "    #print('check 3 {}'.format(temp.shape))\n",
    "    \n",
    "    return temp[[*output.columns,'DELTA_CASE_JH_T',\n",
    "            'target_date_2wk','LOG_DELTA_INC_RATE_T_14', 'DELTA_CASE_SMOOTHED_T_14', 'DELTA_CASE_JH_T_14',\n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21', 'DELTA_CASE_SMOOTHED_T_21', 'DELTA_CASE_JH_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28', 'DELTA_CASE_SMOOTHED_T_28', 'DELTA_CASE_JH_T_28']]\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "df_lagged_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_lagged_list.append(add_lagged_y(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    end_date -= timedelta(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged = pd.concat(df_lagged_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.shape, df_lagged.shape[0]/3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.to_csv('./output/all_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
